{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kTI_jxV_7R56"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from time import time\n",
        "# from keras.models import Sequential, load_model\n",
        "# from keras.layers.core import Dense, Dropout \n",
        "# from keras.layers import LSTM\n",
        "# from time import time\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from pathlib import Path\n",
        "\n",
        "# import nltk\n",
        "# import regex as re\n",
        "# from collections import defaultdict\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "# from nltk.stem.snowball import EnglishStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZzV9t8t87ZOw"
      },
      "outputs": [],
      "source": [
        "# We might have to change the following\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 40, \n",
        "    \"batch_size\": 4,\n",
        "    \"train_p\": 0.55,\n",
        "    \"val_p\": 0.05,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'tanh',\n",
        "    \"timesteps\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "    def __init__(self, path):\n",
        "        self.path = path # Path to the dataset\n",
        "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
        "\n",
        "        self.context_window = 3 # Context window size\n",
        "        self.w2v_feature_vector = []\n",
        "\n",
        "    def read_dataset(self):\n",
        "        \"\"\"\n",
        "        Reads the dataset from the given path.\n",
        "        \"\"\"\n",
        "        ret = pd.read_csv(self.path)\n",
        "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "        \n",
        "        return ret\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the text data.\n",
        "        \"\"\"\n",
        "        self.data['tokens'] = self.data['joke'].apply(word_tokenize) # tokenize the text but keep the punctuation\n",
        "\n",
        "    # get the maximum size of tokens in the dataset and add to column\n",
        "    def get_max_tokens(self):\n",
        "        self.data['max_tokens'] = self.data['tokens'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "    def construct_word2vec(self, max_length):\n",
        "        \"\"\"\n",
        "        Constructs the word2vec model. (Feature vector)\n",
        "        \"\"\"\n",
        "        self.w2v_feature_vector = []\n",
        "        context_words = [] # Construct window list for word2vec\n",
        "        \n",
        "        for line in self.data['tokens']:\n",
        "            for index, word in enumerate(line):\n",
        "                if self.context_window > 0:\n",
        "                    left = index - self.context_window//2\n",
        "                    right = index + self.context_window//2 + 1\n",
        "                else:\n",
        "                    left = index - self.context_window//2\n",
        "                    right = index + self.context_window//2\n",
        "                context_words.append([line[i] for i in range(left, right) if i >= 0 and i < len(line)])\n",
        " \n",
        "\n",
        "        # Create a word2vec model\n",
        "        # context_words = [['a', 'b'], ['a', 'b', 'c'], ['b', 'c', 'd'], ['c', 'd', 'e'], ['d', 'e']] -> list of lists of words and window size is 5\n",
        "        # vector_size = 50 -> dimension of the feature vector (pairs)\n",
        "        # min_count = 4 -> minimum number of occurrences of a word in the corpus\n",
        "        # workers = 4 -> number of threads to use\n",
        "        # window = 5 -> window size\n",
        "        model = Word2Vec(context_words, vector_size=max_length, window=self.context_window, workers=4)\n",
        "\n",
        "        for line in self.data['tokens']:\n",
        "            for index, word in enumerate(line):\n",
        "                if word in model.wv.key_to_index:\n",
        "                    self.w2v_feature_vector.append(model.wv.get_vector(word))\n",
        "                else:\n",
        "                    # if the word is not in the model, then add zero. \n",
        "                    self.w2v_feature_vector.append(np.zeros(max_length))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJL35QEBy-Jz",
        "outputId": "2151be98-c647-49af-d7d4-76e7a6ea5fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "405\n",
            "['My', '5', 'year', 'old', 'just', 'ran', 'out', 'of', 'her', 'room', 'to', 'tell', 'me', 'this', 'joke', 'she', 'just', 'thought', 'up', ':', 'what', 'did', 'the', 'cow', 'say', 'after', 'he', 'was', 'fed', '?', 'Moooooooooore', '!', 'I', \"'ve\", 'never', 'been', 'this', 'proud', 'of', 'anything', 'in', 'my', 'life', '.']\n",
            "[ 1.13410121e-02  6.36864156e-02  1.31977141e-01 -8.45313370e-02\n",
            "  1.94486275e-01  1.29819727e-02  1.71384156e-01  3.88608612e-02\n",
            " -1.78204372e-03  1.57712653e-04 -9.69445740e-04 -5.90142347e-02\n",
            "  1.83116078e-01  5.68029173e-02 -1.81474730e-01 -1.52510896e-01\n",
            "  1.06074832e-01 -8.47889483e-02 -3.03933471e-02 -2.81293634e-02\n",
            " -1.08317055e-01  3.90684977e-02  1.58262193e-01  5.51112220e-02\n",
            "  5.53292222e-02  4.84247953e-02 -1.11941822e-01 -1.67272300e-01\n",
            " -1.84663348e-02 -5.32001816e-02 -8.83795843e-02  1.90830929e-03\n",
            "  4.14039418e-02 -1.38967395e-01  8.77025723e-03  9.60527137e-02\n",
            " -1.96031705e-02 -1.25624269e-01 -5.74848838e-02 -1.25437498e-01\n",
            " -4.73934412e-02 -2.45636553e-02  1.16559446e-01  7.29724318e-02\n",
            " -5.57384044e-02  1.64324995e-02 -1.58726528e-01  1.25474766e-01\n",
            "  5.13848662e-02  1.25931144e-01 -8.23091492e-02 -5.80356494e-02\n",
            " -2.00304627e-01 -6.73649833e-02 -2.61064842e-02 -1.97168291e-01\n",
            "  8.69533792e-02  1.32921487e-01  1.90196395e-01 -6.09316565e-02\n",
            "  5.94976991e-02 -5.96028455e-02  5.96574955e-02 -4.30834591e-02\n",
            " -7.61141479e-02 -2.67427087e-01 -1.01816589e-02  2.08913889e-02\n",
            " -2.40813494e-02 -1.38979346e-01  1.03734553e-01  2.68622458e-01\n",
            "  1.38444200e-01 -1.40384482e-02  2.16705501e-01  7.98436999e-02\n",
            "  2.45647151e-02  1.05389893e-01 -1.38833135e-01  5.93217537e-02\n",
            "  2.49307193e-02  1.49254382e-01  6.88116923e-02 -2.06876278e-01\n",
            "  7.12681701e-03  5.19484542e-02  8.00517872e-02  2.31192067e-01\n",
            " -5.65201342e-02 -1.14178769e-01 -9.75474119e-02 -7.61201903e-02\n",
            " -1.72209084e-01 -8.51574913e-02 -3.30246985e-02  5.38998879e-02\n",
            " -3.60542834e-02 -1.41867578e-01 -1.00591958e-01  6.93427250e-02\n",
            "  7.65919387e-02  1.37517512e-01 -1.19895525e-01 -7.07030743e-02\n",
            "  9.85891744e-02  1.51723875e-02 -1.05130170e-02  2.49102246e-03\n",
            " -1.57849386e-01 -4.38837446e-02  5.14168702e-02  2.87914518e-02\n",
            "  2.81135608e-02 -1.30494893e-01 -4.09783944e-02  1.63339674e-01\n",
            " -1.27566248e-01  6.79212809e-02  1.21008888e-01 -1.52884081e-01\n",
            "  2.62676869e-02 -2.91463546e-02  1.52255027e-02  8.09037015e-02\n",
            " -3.95848230e-02  2.10399516e-02 -7.70645067e-02 -1.35787606e-01\n",
            " -7.84474015e-02  9.03343186e-02  1.30543470e-01 -1.67717994e-03\n",
            " -6.43223226e-02 -2.47775242e-02 -1.26697440e-02  1.51357695e-01\n",
            "  1.44981071e-01 -1.00423865e-01  4.42730859e-02 -8.49039014e-03\n",
            "  3.43358554e-02 -8.49103704e-02 -4.86697592e-02 -6.93110973e-02\n",
            "  1.57791693e-02  1.52401149e-01 -5.56392707e-02  1.24727316e-01\n",
            "  2.72179935e-02 -8.15650299e-02  1.58759058e-01  2.12801963e-01\n",
            "  6.22891961e-03  6.78237304e-02 -9.76459980e-02  5.82121499e-02\n",
            "  7.06379414e-02  1.43595770e-01  5.26962839e-02  1.66692242e-01\n",
            "  3.25805806e-02  1.03455871e-01  2.25175902e-01 -2.03412607e-01\n",
            "  1.16761122e-02 -1.86098471e-01  1.11429073e-01 -1.72988430e-03\n",
            "  1.07712567e-01  1.26644701e-01 -9.14481580e-02  3.27023864e-02\n",
            " -1.28760263e-02  6.37912974e-02  8.55169967e-02  7.56492093e-02\n",
            " -1.65934458e-01  7.91763589e-02 -1.42537713e-01  1.50075182e-02\n",
            "  6.04072176e-02  9.50182453e-02 -7.17596039e-02  1.57280222e-01\n",
            " -1.39780983e-01  1.68839648e-01  4.71115969e-02  8.46781731e-02\n",
            "  9.59083214e-02 -4.32450511e-02  1.11058779e-01 -8.70519578e-02\n",
            " -8.43795831e-04 -3.19713317e-02 -1.52900308e-01  1.56597674e-01\n",
            " -5.44606075e-02  1.37252837e-01  2.31138244e-02  8.32727104e-02\n",
            "  3.00646648e-02 -8.72017443e-02  1.29537471e-02 -9.11917165e-02\n",
            " -4.97925691e-02  1.77493155e-01 -3.45084071e-02 -1.04617178e-01\n",
            "  1.65943384e-01  3.17103900e-02 -8.93218443e-02 -1.74491648e-02\n",
            " -3.72809544e-02  5.22969514e-02  9.33643878e-02 -1.30670279e-01\n",
            " -1.30543366e-01  1.07808523e-01  2.66414165e-01 -2.17768773e-01\n",
            " -2.30173618e-02  2.82353401e-01 -5.99278994e-02 -2.31000911e-02\n",
            "  5.90217561e-02 -3.32492404e-02  1.67062394e-02 -2.15967912e-02\n",
            "  1.42131830e-02 -7.65983686e-02  3.81036513e-02  3.90861072e-02\n",
            " -1.84564710e-01 -6.58535212e-02  1.36682577e-02  2.01890796e-01\n",
            " -8.22294503e-02 -4.92095277e-02 -2.04129994e-01 -4.92882952e-02\n",
            " -2.31838867e-01 -8.67302939e-02 -4.58371043e-02  7.12549537e-02\n",
            "  1.92337893e-02 -7.83843622e-02 -1.40430331e-01 -3.15627269e-02\n",
            " -7.64177591e-02 -9.15623754e-02 -1.39057934e-01 -1.42712623e-01\n",
            " -2.91368067e-01 -1.94763497e-01 -1.34672955e-01 -1.16553143e-01\n",
            " -2.43632868e-01 -8.97545717e-04 -2.58179694e-01 -7.58399144e-02\n",
            "  1.47981554e-01 -5.54085821e-02 -1.77569445e-02  8.88160840e-02\n",
            " -3.32147889e-02  1.57797128e-01  5.99210188e-02 -1.27843678e-01\n",
            " -1.02837510e-01  2.22341623e-02  7.28467181e-02  2.26264335e-02\n",
            " -3.79061475e-02  1.79991946e-01  6.97240010e-02  1.65647000e-01\n",
            "  4.23363559e-02 -1.30293565e-03  1.98656350e-01  1.97095618e-01\n",
            "  9.95634124e-02  3.27151045e-02 -5.33039756e-02  1.06100276e-01\n",
            " -2.00551376e-01 -2.45597303e-01  7.32178316e-02 -2.30178833e-01\n",
            "  3.79947834e-02  1.67479679e-01 -2.78162211e-02  2.10071445e-01\n",
            " -4.01612720e-04 -1.01137929e-01 -1.76166259e-02 -8.04552138e-02\n",
            "  6.09873980e-02  1.03910781e-01  1.15445979e-01 -2.41648585e-01\n",
            "  4.31093946e-02  9.55466330e-02 -1.78016722e-01 -1.93300411e-01\n",
            " -7.04642907e-02 -1.16092920e-01 -6.53911103e-03  3.52189392e-02\n",
            "  1.07358247e-01 -2.91615147e-02 -7.90389553e-02 -1.78859681e-02\n",
            "  3.91836390e-02  2.45179166e-03 -1.29732981e-01  8.74416903e-02\n",
            " -1.07284211e-01 -7.15612695e-02  3.21232677e-02 -8.62359330e-02\n",
            " -7.68974945e-02  7.05314204e-02  2.34107357e-02  2.16854922e-02\n",
            "  3.85910133e-03 -2.98801195e-02 -2.65068561e-02 -2.25341827e-01\n",
            " -7.24139363e-02  8.05021599e-02 -2.29373366e-01 -9.92374644e-02\n",
            " -8.53424892e-02 -8.56844336e-02  4.38314825e-02 -6.36095703e-02\n",
            "  1.24116182e-01 -2.18147457e-01  3.13692316e-02  3.44877094e-01\n",
            "  2.65590716e-02 -5.03521375e-02 -3.62485871e-02  4.14263345e-02\n",
            "  7.36860335e-02  6.98634014e-02 -1.51852757e-01 -9.40582380e-02\n",
            " -1.13547064e-01 -4.33529494e-03  1.23320937e-01  8.78812522e-02\n",
            "  3.68977012e-03 -6.47173524e-02  2.20726039e-02 -1.38530657e-01\n",
            "  2.93290075e-02  5.84094040e-02  1.99816719e-01 -4.89250757e-02\n",
            "  7.15237036e-02 -1.63385734e-01  9.23601165e-02 -1.48510143e-01\n",
            " -7.52779916e-02  8.95969421e-02  1.07985556e-01  1.40542164e-01\n",
            " -3.05432081e-02 -3.04321975e-01 -1.37951881e-01 -6.22497238e-02\n",
            " -1.54110178e-01  1.74475580e-01 -9.28620547e-02 -5.42083122e-02\n",
            " -3.74403633e-02 -1.10902734e-01 -1.40506715e-01  1.38053983e-01\n",
            " -1.17703907e-01  1.70547277e-01  2.48422652e-01  1.86760157e-01\n",
            " -7.15903491e-02  1.50179595e-01 -6.25485629e-02 -9.34248194e-02\n",
            "  1.57751178e-03 -1.31227508e-01  6.79938868e-02 -2.44245559e-01\n",
            "  1.58168063e-01 -2.32412800e-01  2.64842361e-01 -1.08163744e-01\n",
            "  4.67302985e-02  7.74153471e-02 -6.91563860e-02  8.74152407e-02\n",
            " -1.73825368e-01 -9.55301821e-02 -1.59530342e-01 -1.09557435e-03\n",
            " -1.93823218e-01]\n"
          ]
        }
      ],
      "source": [
        "# SETTINGS for local machine - change this for Goolg Colab\n",
        "path = \"dataset/final_jokes(1283).csv\" #\"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.data = joke_model.read_dataset()\n",
        "joke_model.preprocess_text()\n",
        "joke_model.get_max_tokens() # get the maximum number of tokens. Since we need the word2vec feature vector to be of the same size for all jokes. \n",
        "max_length_joke = joke_model.data['max_tokens'].max()\n",
        "print(max_length_joke)\n",
        "joke_model.construct_word2vec(max_length_joke)\n",
        "\n",
        "# print(joke_model.data['tokens'][0])\n",
        "# print(joke_model.w2v_feature_vector[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJ+LSjkMXZfSgYMmoTLpK7",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1j9kXqN1mbP4p8UZmRZX4T4ZLOsXsnFnk",
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
