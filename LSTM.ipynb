{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kTI_jxV_7R56"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from time import time\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM\n",
        "# from time import time\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from pathlib import Path\n",
        "\n",
        "# import nltk\n",
        "# import regex as re\n",
        "# from collections import defaultdict\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# import Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# from nltk.stem.snowball import EnglishStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZzV9t8t87ZOw"
      },
      "outputs": [],
      "source": [
        "# We might have to change the following\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 40, \n",
        "    \"batch_size\": 4,\n",
        "    \"train_p\": 0.55,\n",
        "    \"val_p\": 0.05,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'tanh',\n",
        "    \"timesteps\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "    def __init__(self, path):\n",
        "        self.path = path # Path to the dataset\n",
        "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
        "\n",
        "        self.context_window = 3 # Context window size\n",
        "        self.w2v_feature_vector = []\n",
        "\n",
        "        self.jokes_to_numerical = []\n",
        "\n",
        "    def read_dataset(self):\n",
        "        \"\"\"\n",
        "        Reads the dataset from the given path.\n",
        "        \"\"\"\n",
        "        ret = pd.read_csv(self.path)\n",
        "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "        \n",
        "        return ret\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the text data.\n",
        "        \"\"\"\n",
        "        self.data['tokens'] = self.data['joke'].apply(word_tokenize) # tokenize the text but keep the punctuation\n",
        "\n",
        "    # get the maximum size of tokens in the dataset and add to column\n",
        "    def get_max_tokens(self):\n",
        "        self.data['max_tokens'] = self.data['tokens'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "    def construct_word2vec(self, max_length):\n",
        "        \"\"\"\n",
        "        Constructs the word2vec model. (Feature vector)\n",
        "        \"\"\"\n",
        "        self.w2v_feature_vector = []\n",
        "        context_words = [] # Construct window list for word2vec\n",
        "        \n",
        "        for line in self.data['tokens']:\n",
        "            for index, word in enumerate(line):\n",
        "                if self.context_window > 0:\n",
        "                    left = index - self.context_window//2\n",
        "                    right = index + self.context_window//2 + 1\n",
        "                else:\n",
        "                    left = index - self.context_window//2\n",
        "                    right = index + self.context_window//2\n",
        "                context_words.append([line[i] for i in range(left, right) if i >= 0 and i < len(line)])\n",
        " \n",
        "\n",
        "        # Create a word2vec model\n",
        "        # context_words = [['a', 'b'], ['a', 'b', 'c'], ['b', 'c', 'd'], ['c', 'd', 'e'], ['d', 'e']] -> list of lists of words and window size is 5\n",
        "        # vector_size = 50 -> dimension of the feature vector (pairs)\n",
        "        # min_count = 4 -> minimum number of occurrences of a word in the corpus\n",
        "        # workers = 4 -> number of threads to use\n",
        "        # window = 5 -> window size\n",
        "        model = Word2Vec(context_words, vector_size=max_length, window=self.context_window, workers=4)\n",
        "\n",
        "        for line in self.data['tokens']:\n",
        "            for index, word in enumerate(line):\n",
        "                if word in model.wv.key_to_index:\n",
        "                    self.w2v_feature_vector.append(model.wv.get_vector(word))\n",
        "                else:\n",
        "                    # if the word is not in the model, then add zero. \n",
        "                    self.w2v_feature_vector.append(np.zeros(max_length))\n",
        "\n",
        "\n",
        "    def convert_jokes_to_numerical(self):\n",
        "        \"\"\"\n",
        "        Converts the jokes to numerical values.\n",
        "        \"\"\"\n",
        "        tokenizer = Tokenizer(num_words=None, split=' ')\n",
        "        tokenizer.fit_on_texts(self.data['joke'].values)\n",
        "        self.jokes_to_numerical = tokenizer.texts_to_sequences(self.data['joke'].values)\n",
        "\n",
        "    def pad_sequences(self, max_length):\n",
        "        \"\"\"\n",
        "        Pads the sequences.\n",
        "        \"\"\"\n",
        "        self.jokes_to_numerical = pad_sequences(self.jokes_to_numerical, maxlen=max_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJL35QEBy-Jz",
        "outputId": "2151be98-c647-49af-d7d4-76e7a6ea5fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of joke:  405\n"
          ]
        }
      ],
      "source": [
        "# SETTINGS for local machine - change this for Goolg Colab\n",
        "path = \"dataset/final_jokes(1283).csv\" #\"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.data = joke_model.read_dataset()\n",
        "\n",
        "joke_model.preprocess_text()\n",
        "joke_model.get_max_tokens() # get the maximum number of tokens. Since we need the word2vec feature vector to be of the same size for all jokes. \n",
        "max_length_joke = joke_model.data['max_tokens'].max()\n",
        "print(\"Max length of joke: \", max_length_joke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FIRST METHOD: word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "joke_model.construct_word2vec(max_length_joke)\n",
        "# print(len(joke_model.data['tokens'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECOND METHOD: Tokenizer from keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of first line:  186\n",
            "Length of feature vector after normalisation:  405\n"
          ]
        }
      ],
      "source": [
        "joke_model.convert_jokes_to_numerical()\n",
        "print(\"Length of first line: \", len(joke_model.data['joke'][0]))\n",
        "\n",
        "joke_model.pad_sequences(max_length_joke)\n",
        "print(\"Length of feature vector after normalisation: \", len(joke_model.jokes_to_numerical[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.01190722  0.05554788  0.11459028 -0.1150763   0.22286381 -0.0155257\n",
            "  0.16852403  0.02064565  0.01578335 -0.02260887  0.02366269 -0.09488297\n",
            "  0.14833543  0.05778171 -0.19579011 -0.17012028  0.13127667 -0.08018813\n",
            " -0.06836197 -0.02995019 -0.12100121  0.02024477  0.13432936  0.05154734\n",
            "  0.06854516  0.09863234 -0.12623179 -0.1973311  -0.00601147 -0.04878491\n",
            " -0.09182403 -0.0076752   0.04144758 -0.10919733  0.02863755  0.08945033\n",
            " -0.01912385 -0.11983074 -0.03082166 -0.10574423 -0.05163813 -0.00842052\n",
            "  0.13482538  0.06638224 -0.04382432  0.01843091 -0.16098052  0.12893982\n",
            "  0.04196787  0.10164028 -0.08608922 -0.07912943 -0.21943185 -0.09881731\n",
            " -0.02099738 -0.26242748  0.07640062  0.14756086  0.15768601 -0.09466252\n",
            "  0.06271809 -0.08707754  0.09271543 -0.0210466  -0.07002075 -0.26492184\n",
            " -0.01147205  0.03388357  0.00128775 -0.16882087  0.10245993  0.28725275\n",
            "  0.16894642  0.02838083  0.21739523  0.07468344  0.05968522  0.10323022\n",
            " -0.17828791  0.04917506  0.04343579  0.19628695  0.05871369 -0.19540909\n",
            "  0.0108205   0.05013     0.0766089   0.2196668  -0.06648539 -0.07785701\n",
            " -0.06849484 -0.08024552 -0.15902065 -0.05324458 -0.02175952  0.05123084\n",
            " -0.03356281 -0.1507383  -0.09971238  0.04661672  0.08609818  0.10225042\n",
            " -0.13196799 -0.08679898  0.10342806  0.0368637   0.00229501 -0.0025442\n",
            " -0.16480997 -0.04868915  0.06068615  0.01032961  0.04116198 -0.1275063\n",
            " -0.0288272   0.16695763 -0.13283971  0.08652029  0.15101428 -0.16338988\n",
            "  0.03486907 -0.03996285 -0.01224783  0.10098116 -0.02067635  0.01459572\n",
            " -0.05690551 -0.12646082 -0.0759738   0.10728489  0.13091116 -0.00984186\n",
            " -0.08104096 -0.05409715 -0.01480593  0.15534388  0.14907171 -0.1215359\n",
            "  0.04512113 -0.04100126  0.02835685 -0.07318828 -0.06019452 -0.05847854\n",
            "  0.00619171  0.12683247 -0.06674531  0.09578272  0.0061368  -0.08982687\n",
            "  0.1789588   0.20708096 -0.03614164  0.04949582 -0.08951769  0.03425999\n",
            "  0.09533542  0.1652101   0.06466494  0.1816449   0.05450286  0.11932981\n",
            "  0.23201814 -0.20436484  0.02767405 -0.18519747  0.12229235  0.02615054\n",
            "  0.09101384  0.13035746 -0.07376209  0.01551636  0.02501632  0.03703182\n",
            "  0.09897526  0.05145879 -0.12918824  0.11387966 -0.15114379  0.01174276\n",
            "  0.09022326  0.09655534 -0.05084099  0.17665942 -0.12147573  0.19772466\n",
            "  0.05449648  0.09367389  0.12179613 -0.02821556  0.14727539 -0.07388601\n",
            " -0.00577915 -0.03626284 -0.15109767  0.187595   -0.06723254  0.12025188\n",
            "  0.03128282  0.08692082  0.03375888 -0.10088483  0.01826235 -0.10143574\n",
            " -0.03908304  0.15880688 -0.02846458 -0.09235922  0.16218545  0.05466831\n",
            " -0.07753223  0.01540581 -0.05016342  0.02953556  0.08889514 -0.14019546\n",
            " -0.14835986  0.11637458  0.2507974  -0.23204473 -0.02813822  0.31581724\n",
            " -0.06512979  0.00554242  0.01948275 -0.02271646 -0.0053335  -0.0336448\n",
            "  0.02571321 -0.05967936  0.04965843  0.0479675  -0.18605395 -0.09718768\n",
            "  0.02751745  0.19644602 -0.0629084  -0.02721158 -0.19401985 -0.08247247\n",
            " -0.240046   -0.10246024 -0.05035429  0.07326443  0.01195017 -0.09807283\n",
            " -0.1683322  -0.02912981 -0.07625698 -0.0981967  -0.12313673 -0.1844863\n",
            " -0.284112   -0.18515056 -0.13018888 -0.1081775  -0.24991906  0.02007068\n",
            " -0.28674868 -0.07916269  0.16428763 -0.0421714  -0.0098251   0.0907314\n",
            " -0.04159258  0.16155611  0.02985928 -0.13995717 -0.08183626  0.02508757\n",
            "  0.06428626  0.004362   -0.04245433  0.20424666  0.06877616  0.17670134\n",
            "  0.00858746  0.00362626  0.1745854   0.22880852  0.07345878  0.05308114\n",
            " -0.04136903  0.09089357 -0.2097052  -0.24952582  0.07527042 -0.2497511\n",
            "  0.02650635  0.19115958 -0.03790428  0.21189661  0.04590512 -0.12788256\n",
            " -0.04293177 -0.07456773  0.03866958  0.10694324  0.1392354  -0.23410514\n",
            "  0.05834597  0.10279856 -0.17702156 -0.20679529 -0.07936489 -0.1368893\n",
            "  0.00185767  0.01783283  0.1034437  -0.03275255 -0.11187302 -0.00841998\n",
            "  0.05464425  0.00337701 -0.12796806  0.10227784 -0.0964888  -0.06510512\n",
            "  0.01916985 -0.09228253 -0.07560968  0.10482872  0.03718837  0.03130258\n",
            " -0.01195919 -0.03600716 -0.00523568 -0.2256839  -0.04544139  0.06625156\n",
            " -0.22472286 -0.14765456 -0.10020537 -0.0738204   0.05073148 -0.04367448\n",
            "  0.14743474 -0.23499225  0.02545518  0.33639723  0.02121002 -0.02980515\n",
            " -0.08183425  0.0594171   0.05937404  0.05122422 -0.13448355 -0.09834114\n",
            " -0.10212021 -0.01086621  0.14086804  0.08075748 -0.00743615 -0.05762212\n",
            "  0.04341789 -0.18753703  0.00220536  0.07416487  0.23068139 -0.05208351\n",
            "  0.06293447 -0.20028633  0.09473772 -0.13905191 -0.07377804  0.12021056\n",
            "  0.11420123  0.14344408 -0.03752209 -0.26205316 -0.12364835 -0.05624495\n",
            " -0.1435196   0.17124276 -0.10595921 -0.04723889 -0.04046762 -0.09868454\n",
            " -0.14182915  0.13420711 -0.13930967  0.14816155  0.2239185   0.18848413\n",
            " -0.08446324  0.14692095 -0.08629803 -0.105079    0.00460774 -0.15128784\n",
            "  0.07278284 -0.24103516  0.12640958 -0.27143627  0.28190595 -0.13559411\n",
            "  0.01813343  0.1248816  -0.05406482  0.10193805 -0.1601648  -0.10319877\n",
            " -0.17570741  0.00196545 -0.19725321]\n"
          ]
        }
      ],
      "source": [
        "print(joke_model.w2v_feature_vector[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJ+LSjkMXZfSgYMmoTLpK7",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1j9kXqN1mbP4p8UZmRZX4T4ZLOsXsnFnk",
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
