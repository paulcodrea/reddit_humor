{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "kTI_jxV_7R56"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from time import time\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM\n",
        "# from time import time\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from pathlib import Path\n",
        "\n",
        "# import nltk\n",
        "# import regex as re\n",
        "# from collections import defaultdict\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# import Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# from nltk.stem.snowball import EnglishStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZzV9t8t87ZOw"
      },
      "outputs": [],
      "source": [
        "# We might have to change the following\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 40, \n",
        "    \"batch_size\": 4,\n",
        "    \"train_p\": 0.55,\n",
        "    \"val_p\": 0.05,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'tanh',\n",
        "    \"timesteps\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "    def __init__(self, path):\n",
        "        self.path = path # Path to the dataset\n",
        "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
        "\n",
        "        self.context_window = 3 # Context window size\n",
        "        self.w2v_feature_vector = []\n",
        "\n",
        "        self.jokes_to_numerical = []\n",
        "\n",
        "    def read_dataset(self):\n",
        "        \"\"\"\n",
        "        Reads the dataset from the given path.\n",
        "        \"\"\"\n",
        "        ret = pd.read_csv(self.path)\n",
        "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "        \n",
        "        return ret\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the text data.\n",
        "        \"\"\"\n",
        "        self.data['tokens'] = self.data['joke'].apply(word_tokenize) # tokenize the text but keep the punctuation\n",
        "\n",
        "    # get the maximum size of tokens in the dataset and add to column\n",
        "    def get_max_tokens(self):\n",
        "        self.data['max_tokens'] = self.data['tokens'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "    def construct_word2vec(self, max_length):\n",
        "        \"\"\"\n",
        "        Constructs the word2vec model. (Feature vector)\n",
        "        \"\"\"\n",
        "        self.w2v_feature_vector = []\n",
        "        context_words = [] # Construct window list for word2vec\n",
        "        \n",
        "        for line in self.data['tokens']:\n",
        "            for index, word in enumerate(line):\n",
        "                if self.context_window > 0:\n",
        "                    left = index - self.context_window//2\n",
        "                    right = index + self.context_window//2 + 1\n",
        "                else:\n",
        "                    left = index - self.context_window//2\n",
        "                    right = index + self.context_window//2\n",
        "                context_words.append([line[i] for i in range(left, right) if i >= 0 and i < len(line)])\n",
        " \n",
        "\n",
        "        # Create a word2vec model\n",
        "        # context_words = [['a', 'b'], ['a', 'b', 'c'], ['b', 'c', 'd'], ['c', 'd', 'e'], ['d', 'e']] -> list of lists of words and window size is 5\n",
        "        # vector_size = 50 -> dimension of the feature vector (pairs)\n",
        "        # min_count = 4 -> minimum number of occurrences of a word in the corpus\n",
        "        # workers = 4 -> number of threads to use\n",
        "        # window = 5 -> window size\n",
        "        model = Word2Vec(context_words, vector_size=max_length, window=self.context_window, workers=4)\n",
        "\n",
        "        for line in self.data['tokens']:\n",
        "            for index, word in enumerate(line):\n",
        "                if word in model.wv.key_to_index:\n",
        "                    self.w2v_feature_vector.append(model.wv.get_vector(word))\n",
        "                else:\n",
        "                    # if the word is not in the model, then add zero. \n",
        "                    self.w2v_feature_vector.append(np.zeros(max_length))\n",
        "\n",
        "\n",
        "    def convert_jokes_to_numerical(self):\n",
        "        \"\"\"\n",
        "        Converts the jokes to numerical values.\n",
        "        \"\"\"\n",
        "        tokenizer = Tokenizer(num_words=None, split=' ')\n",
        "        tokenizer.fit_on_texts(self.data['joke'].values)\n",
        "        self.jokes_to_numerical = tokenizer.texts_to_sequences(self.data['joke'].values)\n",
        "\n",
        "    def pad_sequences(self, max_length):\n",
        "        \"\"\"\n",
        "        Pads the sequences.\n",
        "        \"\"\"\n",
        "        self.jokes_to_numerical = pad_sequences(self.jokes_to_numerical, maxlen=max_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJL35QEBy-Jz",
        "outputId": "2151be98-c647-49af-d7d4-76e7a6ea5fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of joke:  405\n"
          ]
        }
      ],
      "source": [
        "# SETTINGS for local machine - change this for Goolg Colab\n",
        "path = \"dataset/final_jokes(1283).csv\" #\"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.data = joke_model.read_dataset()\n",
        "\n",
        "joke_model.preprocess_text()\n",
        "joke_model.get_max_tokens() # get the maximum number of tokens. Since we need the word2vec feature vector to be of the same size for all jokes. \n",
        "max_length_joke = joke_model.data['max_tokens'].max()\n",
        "print(\"Max length of joke: \", max_length_joke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FIRST METHOD: word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.', 'a', 'the', '?', ',', 'I', 'to', 'you', 'it', 'do', 'and', 'of', \"'s\", 'in', '’', 'was', 'is', 'What', 'my', '!', 'that', 'did', 'he', \"n't\", 'with', 'A', '``', 'call', 'me', '...', 'for', ':', \"''\", 'The', 'on', 'about', 'his', 'It', 'an', 'they', 'have', 's', 'get', ';', 'He', 'are', 'Why', 'this', 'My', 'who', 'when', 'so', 'but', 'say', 'can', '“', 'does', 'up', 'at', '&', 'be', 't', 'amp', 'said', '”', 'They', 'like', 'out', 'one', 'into', 'just', 'joke', 'what', 'she', \"'m\", 'Because', 'all', 'had', 'know', 'got', 'man', 'from', 'if', 'there', 'hear', 'favorite', 'were', 'them', 'because', 'wife', 'her', 'your', 'called', 'not', 'no', 'But', 'You', 'says', 'told', 'him', 'friend', 'down', 'How', 'would', 'by', 'could', 'always', 'asked', \"'re\", 'has', 'go', '(', 'as', '*', 'make', 'how', 'going', '%', 'their', 'see', 'really', 'why', 'So', 'm', 'other', 'new', 'heard', 'never', 'two', 'time', 'If', ')', 'went', 'people', 'car', 'When', 'we', 'ca', 'bar', 'dad', 'Did', 'day', 'dog', '#', 'guy', 'made', 'jokes', 'No', 'bad', 'some', 'back', 'name', 'take', 'Me', 'This', 'over', 'don', 'kind', 'think', 'good', 'water', 'saw', 'She', 'been', 'then', 'Corona', 'first', 'best', 'after', \"'ve\", 'walks', 'than', 'very', 'between', 'should', 'someone', 'old', 'funny', 'lost', 'tell', 'well', 'x200B', 'only', \"'\", 'Now', 'many', 'long', 'today', 'more', 'house', 'eat', 'na', 'thought', 'want', 'That', 'To', 'There', 'i', 'off', 'little', 'replied', 'stop', 'son', 'difference', 'way', 'cow', 'wanted', 're', 'find', 'job', \"'ll\", 'year', '....', 'play', 'great', 'night', 'or', 'through', 'too', 'guess', 'One', 'last', 'here', 'cross', 'before', 'We', 'most', 'away', 'doing', 'still', 'now', 'https', 'work', 'drink', 'bought', 'bear', 'asks', 'will', '..', 'Have', 'where', 'also', 'girlfriend', 'getting', 'road', 'give', 'German', 'book', 'kids', 'our', 'use', 'gon', 'person', 'body', 've', 'started', 'doctor', 'll', 'shit', 'which', 'school', 'came', 'walk', 'died', '2', 'Do', 'Who', 'song', 'looks', 'friends', '‘', 'around', 'much', 'Where', 'found', 'line', \"'d\", 'having', 'company', 'without', 'girl', 'while', 'phone', 'everyone', 'horse', 'both', 'saying', 'chicken', 'ask', 'Is', 'An', '1', '[', 'band', 'run', 'lot', 'Well', '5', 'Then', 'woman', 'makes', 'God', 'common', 'd', 'sorry', 'need', 'leave', 'turned', ']', 'place', 'such', 'fish', 'believe', 'matter', 'dead', 'look', 'dumb', 'broke', 'once', 'sure', 'didn', 'something', 'part', 'its', 'Friend', 'Dad', 'Just', 'recently', 'stand', '3', 'took', 'another', 'real', 'cows', 'paper', 'party', 'being', 'In', 'used', 'pizza', 'inside', 'cheese', 'tried', 'Hey', 'fly', 'date', 'pet', 'happens', 'gym', 'sick', 'thing', 'guys', 'anything', 'walking', 'food', 'post', 'even', 'left', 'bartender', 'baby', 'pretty', 'mind', 'store', 'must', 'toilet', 'love', 'making', 'New', 'idea', 'Two', 'Not', 'big', 'goes', 'tired', 'during', 'wear', 'years', 'priest', 'hit', 'come', 'movie', 'And', 'cut', 'fall', 'story', 'Your', 'Italian', 'door', 'everything', 'space', 'high', 'die', 'nothing', 'men', 'Kobe', 'game', 'Russian', 'keep', 'home', 'virus', 'things', 'fuel', 'hot', 'myself', 'actually', 'put', 'M-m-m-my', 'since', 'number', 'ever', 'better', 'right', 'eyes', 'together', 'hospital', 'eating', 'mama', 'laughing', 'beer', 'Man', 'Never', 'sign', 'cat', 'milk', 'restaurant', 'end', 'balls', 'window', 'Which', 'hate', 'kill', 'own', 'different', '-', 'Mercedes', 'walked', 'wrote', 'Chinese', 'three', 'drug', 'At', 'done', 'tank', 'gets', 'head', 'diesel', 'killed', 'forgot', 'dinner', 'em', 'machine', 'daughter', 'us', 'ring', 'His', 'worker', 'side', 'pants', '10', 'decided', 'red', 'change', 'kid', 'remember', 'pirate', 'touch', 'legs', 'worst', 'bathroom', 'wooden', 'coronavirus', 'looking', 'start', 'children', 'letters', 'arrested', '8', 'world', 'nose', 'afraid', 'room', 'giraffe', 'Mexican', 'magician', 'same', 'count', 'looked', 'Johnson', 'seen', 'mean', 'Knock', 'From', 'fruit', 'murder', 'hey', 'truck', 'race', 'office', 'mom', 'every', 'coming', 'hope', 'won', 'paint', 'himself', 'birthday', 'cook', 'become', 'French', 'turns', 'eight', 'snow', 'brother', 'leaving', 'full', 'tractor', 'met', 'salad', 'gay', 'second', 'yesterday', 'share', 'business', 'brown', 'computer', 'Let', 'auto=webp', 'calling', 'wondered', 'pay', 'concert', 'middle', 'Mike', 'blood', 'sent', 'fire', 'word', 'loved', 'Russiаn', 'cold', 'almost', 'ya', 'playing', 'C', 'buys', '\\\\-', 'brought', 'college', 'week', 'face', 'key', 'street', 'instead', 'throw', 'heavy', 'hard', 'tea', 'Jokes', 'catch', 'coffee', 'cream', 'light', '4', '7', 'character', 'played', 'show', 'knew', 'longer', 'happened', 'wrong', 'morning', 'student', 'rock', 'parents', 'Son', 'After', 'Yes', 'Thanks', 'oh', 'anyone', 'Guess', 'Today', 'Q', 'Oh', 'Yo', 'nuts', 'half', 'ate', 'comes', 'steering', 'finally', 'piece', 'quick', 'likes', 'glasses', 'wheel', 'driving', 'attached', 'marriage', 'numbers', 'women', 'trip', 'color', 'tree', 'dragon', 'short', 'accidentally', 'Soviet', 'conversation', 'worry', 'faster', 'fan', 'watching', 'On', 'ran', 'sold', 'stick', 'probably', 'proud', 'life', 'crazy', 'fell', 'puns', 'though', 'sport', 'pregnant', 'fast', 'Was', 'pasta', 'pun', 'rice', 'any', 'let', 'human', 'trees', 'butt', 'wait', 'broken', 'feeling', 'nine', 'nephew', 'foot', 'worked', 'news', '4YO', 'lady', 'few', 'stole', 'money', 'using', 'bit', 'keeps', 'sale', 'terrible', 'poor', 'weird', 'blind', 'John', 'caught', 'shop', '$', '12', 'Bach', 'wearing', 'type', 'husband', 'helping', 'trust', 'quit', 'Space', 'Can', 'picture', 'butter', 'problems', 'spread', 'beef', 'pass', 'hole', 'Greek', 'may', 'invented', 'mine', 'replies', 'talking', 'enough', 'minute', 'singer', 'chef', 'test', 'sitting', 'King', '6', 'factory', 'Schwarzenegger', 'themed', 'books', 'bread', 'trying', 'Our', 'guitar', 'quite', 'MAZ', 'cant', 'question', 'jump', '20', 'format=png', 'disease', 'else', 'hand', 'leader', 'following', 'young', 'clock', 'boat', 'themselves', 'teacher', 'shape', 'whoa', 'Irishman', 'live', 'aye-aye', 'Get', 'im', 'account', 'dirty', 'travel', 'Bad', 'cars', 'vision', 'drinks', 'gas', 'Irish', 'bah', 'again', 'police', 'heart', 'floor', 'mother', 'blue', 'As', 'Easter', 'Islander', 'original', 'gave', 'driver', 'instructor', 'Australian', 'committed', 'pee', 'wee', 'Yeah', 'Eye', 'With', 'ha', 'Take', 'Mario', 'First', 'Americans', 'People', 'Courtesy', 'ok', 'instrument', 'single', 'elephant', 'taste', 'group', 'spell', 'easy', 'am', '9', 'favourite', 'counts', 'meat', 'kinda', 'late', 'giving', 'child', 'Juan', 'thank', 'prisoner', 'wall', '{', 'signs', 'tonight', 'yard', 'Batman', 'opposite', 'bus', 'Wife', 'ghost', 'star', 'chip', 'states', 'York', 'addicted', 'soap', 'roll', 'crash', 'outside', 'fisherman', 'career', 'sleep', 'Hitler', 'train', 'Lauren', 'male', 'stone', 'digging', 'local', 'strings', 'posters', 'alligator', 'Apple', 'spider', 'cents', 'eye', '.....', 'Indian', 'moving', 'text', 'boy', 'close', 'chocolate', 'ice', 'framed', 'able', 'hungry', 'Hamburger', 'sticky', 'F', 'usually', 'bombed', 'burn', 'open', 'cool', 'experienced', 'therapist', 'wouldn', 'cats', 'alphabet', 'clothes', 'spring', 'brothel', 'moon', 'shoes', 'bowl', 'burned', 'temperature', 'might', 'enjoy', 'non', 'grow', 'learned', 'sex', 'British', 'small', 'sexist', 'standing', 'collection', 'resolution', 'widow', 'Swiss', 'prom', 'naked', 'app', 'Arabia', 'boss', 'earth', 'times', 'Cause', 'musician', 'sees', 'Catholic', 'extreme', 'silent', 'main', 'u', 'Nothing', 'k', 'liked', 'reddit', 'Of', 'Michael', 'front', 'Reddit', 'punch', 'sausage', 'wurst', 'large', 'pub', 'god', 'trucks', 'deal', 'Canadian', 'means', 'flying', 'ones', 'completely', 'starts', 'knows', 'penis', 'days', 'motorway', 'online', 'punchline', 'move', 'ground', 'decade', 'engine', 'University', 'act', 'power', 'potatoes', 'Noah', 'speak', 'third', 'jumped', 'yet', 'marijuana', 'Ooooooo-ohhh', 'confused', 'tractors', 'white', 'future', 'happy', 'Amazon', 'nice', 'animals', 'soda', 'air', 'Israeli', 'Does', 'sister', 'stairs', 'shot', 'calls', 'glue', 'return', 'black', 'those', 'weed', 'airport', 'toad', 'perform', 'sun', 'neighbor', 'whale', 'ride', 'calendar', 'frank', 'Prime', 'yelled', 'shoot', 'bird', 'thinks', 'Turns', 'history', 'detective', 'mouth', 'mad', 'dick', 'style', 'ill', 'evidence', '\\\\', 'became', 'trouble', 'system', 'Micheal', 'round', 'doesn', 'tents', 'Pirate', 'drive', 'threw', 'garden', 'lighter', 'these', 'talk', 'Stevie', 'cleaned', 'thinking', 'parish', 'Wonder', 'definitely', 'player', 'kept', 'rolls', 'needed', 'five', 'tennis', 'Roy', 'farm', 'B', 'scent', 'buddy', 'bed', 'help', 'Avgan', 'agent', 'notes', 'language', 'puma', 'sad', 'clean', 'charge', 'pipe', 'bull', 'handed', 'fingers', 'important', 'YOU', 'pew', 'hoes', 'reason', 'partner', 'smoke', 'cowboy', 'ordered', 'lettuce', 'bank', 'math', 'amazing', 'telling', 'meant', 'yeah', 'Come', 'Jessica', 'Jesus', 'Dude', 'Every', 'Time', 'Joke', 'Even', 'Are', '🔥', 'Spring', 'Whats', 'Call', 'Elon', 'Love', 'whats', 'Very', 'Marriage', 'Snow', 'Happy', 'Broken', 'Wan', '}', 'Some', 'Warning', 'hang', '//www.buffalocoresupply.com/blog/a-little-automotive-humor', 'sports', 'soup', 'monkeys', 'ant', 'y', 'illegal', 'fr2=piv-web', 'Hawaiian', 'wants', '3A+Mag-Blok+magnetic+holder', '3A', '2Fimages', 'Daddy', '2F', 'fr=iphone', 'grocery', 'low', 'dyslexic', 'patience', 'vocal', 'dwarf', 'warm', 'scene', 'showed', 'lots', 'condescending', 'passing', 'lovely', 'bigger', 'content', 'passed', 'far', 'theft', 'worth', 'Te', 'birds', 'banned', 'y/o', '14', 'geese', 'hug', 'absolute', 'Rick', 'isn', 'terminal', 'write', 'Telescope', 'falls', 'Crow', 'Russell', 'Scott', 'disorder', 'windows', 'mates', 'parent', 'Scooter', 'pianist', 'Removed', 'personal', 'across', 'chemistry', 'frying', 'display', 'humor', 'Until', 'russian', 'belt', 'yelling', 'solid', 'avoid', 'printing', 'Calvin', 'pins', 'held', 'television', 'woods', 'Sir', 'spelling', 'medication', 'mathematician', 'depression', 'stolen', 'funeral', 'wrinkled', 'anymore', 'N', 'None', 'Wokking', 'ocean', 'Mandalorian', 'Ooh', 'huh', 'apart', 'wedding', 'virgin', 'response', 'rash', 'member', 'ta', 'tho', 'letter', 'jack', 'duck', 'king', 'information', 'ends', 'batteries', 'nachos', 'music', 'Snoop', 'named', 'California', 'recent', 'fatal', 'Harriet', 'worse', 'screams', 'bro', 'here.', 'Funny', 'serve', 'Watch', 'fat', 'Jack', 'popular', 'boys', 'Boy', 'canyon', 'Nova', 'log', 'Champagne', 'float', 'amputees', 'e', 'Sailor', 'Way', 'sip', 'crab', 'fed', 'brain', 'std', 'Stdent', 'Towel', 'fishermen', 'Drift', 'fishing', 'showing', 'reading', 'comedy', 'served', 'Post', 'naturally', '┃┃┃┃', 'waiter', 'tyrannosaurus', 'Christmas', 'whores', 'Epstein', 'Play', 'cakes', 'seafood', 'forth', 'midgets', 'laugh', 'tres', 'station', 'songs', 'Luigi', 'Lime', 'dos', 'Park', 'helicopter', 'smoking', 'true', 'social', 'disappear', 'media', 'bone', 'mulch', 'brothers', 'sits', 'folks', 'More', 'takes', 'early', 'healthy', 'stories', 'Ah', 'attempt', 'surprised', 'wan', 'peanut', 'stoner', 'bounce', 'movement', 'For', 'hearing', 'Schnapp', 'twice', 'cliffs', 'fuck', 'sense', 'Jackson', 'stocking', 'slippery', 'wet', 'shave', 'f', 'Please', 'tie', 'world-record', 'failed', 'wishing', 'set', 'viral', 'iron', 'suffering', 'yourself', 'fucking', 'English', 'snowman', 'credit', 'Reese', 'pets', 'realized', 'record', 'cobble', 'Sia', 'couldn', 'TV', 'nightmare', 'sandwich', 'March', 'lunch', 'launched', 'condition', 'rain', 'taking', 'church', 'clip', 'Dead', 'Trump', 'save', 'next', 'built', 'strong', 'per', 'Divorce', 'Leia', 'Han', 'Sorting', 'Make', 'cabinet', 'time-consuming', 'robot', 'lives', '9/11', 'retarded', 'invisible', 'Alexis', 'golden', 'Vietnamese', 'didnt', 'maths', 'castle', 'park', 'funk', 'Saudi', 'country', 'singing', 'loves', 'struck', 'faith', 'vampire', 'hoe', 'boob', 'quarantine', 'plane', 'Collada', 'sales', '50', 'pitch', 'evil', 'KKK', 'beverage', 'holey', 'prostitute', 'border', 'sea', 'super', 'programmer', 'pencil', 'slide', 'China', 'pulling', 'playground', 'weight', 'mist', 'mommy', 'free', 'DNA', 'daddy', 'kitchen', 'grandkids', 'visit', 'cod', 'orders', 'sore', 'site', 'odd', 'weekend', 'Follow', 'missing', 'horrible', 'poorly', 'suicide', 'wings', 'dressed', 'THE', 'trans', 'mascot', 'programming', 'tuna', 'Will', 'odor', 'skunk', 'stock', 'attacked', '100', 'against', 'changed', 'skipped', 'working', '80s', 'mass', 'IT', 'filled', 'Stephen', 'chemist', 'spent', 'dream', 'starting', 'under', 'minor', 'rocks', 'honey', 'pick', 'Geome', 'Jedi', 'check', 'psychiatrist', 'chow', 'buck', 'Australia', 'ass', '80th', 'Anna', 'Someone', 'dealer', 'fighting', 'coat', 'sticking', 'batch', 'off-road', 'step', 'weapon', 'USB', 'zombies', 'gangsters', 'identifies', 'grows', 'musical', 'Monica', 'ball', 'basketball', 'hurry', 'oven', 'counselor', 'Tom', 'Dishes', 'knock', 'losing', 'sued', 'video', 'opened', '7th', 'becomes', 'fields', 'farmer', 'drinking', 'develop', 'constipation', 'newborn', 'less', 'wasn', 'allowed', 'honest', 'dogs', 'bike', 'pack', 'Wars', 'Star', 'frog', 'class', 'towards', 'vacation', 'Want', 'disappears', 'chameleon', 'Sun', 'Union', 'reptile', 'bills', 'french', 'army', 'near', 'shooting', 'scared', 'chick', 'Lipa', 'extremely', 'cycle', 'falling', 'land', 'breathes', 'known', 'drop', 'shouldn', 'code', 'toys', 'offer', 'Everyone', 'pot', 'case', 'cover', 'Good', 'prison', 'generally', 'eagle', 'younger', 'national', 'dressing', 'kicked', 'fun', 'cleaning', 'worried', 'skin', 'exclusively', 'flight', 'building', 'chill', 'biggest', 'rival', 'gathering', 'knee', 'Rushmore', 'units', 'bill', 'whenever', 'argument', 'ended', 'loss', 'interesting', 'carbon', 'jail', 'toast', 'arson', 'Charles', 'ketchup', 'drove', 'Alaska', 'bends', 'Ones', 'others', 'ago', 'memory', 'counting', 'renamed', 'France', 'bridge', 'watch', 'knight', 'plays', 'Skywalker', 'shows', 'adopting', 'Passover', 'radio', '4/20', 'circus', 'prefer', 'dish', 'Japanese', 'stopped', 'pair', 'Underwater', 'below', 'dear', 'okay', 'mail', 'beach', 'slices', 'dance', 'point', 'four', 'level', 'experience', 'capital', 'sudden', 'composers', 'sleeping', 'miner', 'genius', 'atomic', 'Arnold', 'Damme', 'Van', 'Stallone', 'bunch', 'cheesy', 'sell', 'Brazilian', 'Terrible', 'Jamaican', 'rent', 'Swede', 'Norwegian', 'fine', 'acronym', 'unexpected', 'Breathing', 'Apparatus', 'yell', 'try', 'crotch', 'realize', 'replace', 'Moon', 'creature', 'dads', 'ted', 'hooker', 'yes', 'aware', 'crack', 'wing', 'taught', 'dollars', 'seven', 'organ', 'squared', 'buy', 'solar', 'locksmith', 'programmers', 'Beethoven', 'cute', 'attack', 'lame', 'drops', 'understand', 'construction', 'shady', 'seem', 'stuff', 'direction', 'communication', 'whey', 'jury', 'cost', 'yoga', 'witness', 'dinosaurs', 'hoping', 'steaks', 'million', 'bury', 'gift', 'Owls', 'allergic', 'peanuts', 'feed', 'organs', 'hell', 'sound', 'whinny', 'lobster', 'map', 'cannibals', 'clown', 'strange', 'acid', 'worms', 'stay', 'wheels', 'Wooden', 'thirsty', 'Lisa', 'Iranian', 'couch', 'couple', 'S.', 'bottom', 'sofa', 'iphone', 'Drop', 'rule', 'sit', 'straight', 'firm', 'waiting', 'pane', 'running', 'gone', 'binary', 'cause', 'glass', 'course', 'brake', 'Edit', 'dessert', 'happen', 'serving', 'rumor', 'top', 'fresh', 'answer', 'Russia', 'prawns', 'accident', 'thyme', 'forget', 'guitarist', 'shocked', 'hands', 'chasing', 'alloys', 'teamed', 'sentence', 'rises', 'Egyptian', 'weather', 'daily', 'elevators', '/', 'months', 'sentenced', 'ejaculate', 'divorced', 'age', 'Sorry', 'maker', 'professor', 'haircut', 'dollar', 'woke', 'capitalism', 'either', 'answers', 'along', 'add', 'poured', 'Little', 'bright', 'lamp', 'Frenchman', 'several', 'Actually', 'minutes', 'stated', 'military', 'shares', 'excited', 'Chewbacca', 'feel', 'hiding', 'War', 'Before', 'until', 'actors', '//youtu.be/roj5Xf55PDU', 'paid', 'Great', 'Paul', 'deux', 'trois', 't-shirt', 'raising', 'While', 'All', 'Thank', 'Since', 'Got', 'Despite', '18+', '2020', 'Genie', 'Instead', 'GG', 'Coronavirus', 'Like', 'Walking', 'Check', '😂', 'See', 'Free', 'Capital', 'Paper', 'Dogs', 'Peter', 'Told', 'Idk', 'Czech', 'Birds', 'Helium', 'Earth', 'Eating', 'DAE', 'Found']\n",
            "44\n"
          ]
        }
      ],
      "source": [
        "\n",
        "joke_model.construct_word2vec(max_length_joke)\n",
        "# print(len(joke_model.data['tokens'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECOND METHOD: Tokenizer from keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of first line:  186\n",
            "Length of feature vector after normalisation:  405\n"
          ]
        }
      ],
      "source": [
        "joke_model.convert_jokes_to_numerical()\n",
        "print(\"Length of first line: \", len(joke_model.data['joke'][0]))\n",
        "\n",
        "joke_model.pad_sequences(max_length_joke)\n",
        "print(\"Length of feature vector after normalisation: \", len(joke_model.jokes_to_numerical[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(len(joke_model.data['joke'][0]))\n",
        "# print(len(joke_model.jokes_to_numerical[0]))\n",
        "\n",
        "# joke_model.jokes_to_numerical[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJ+LSjkMXZfSgYMmoTLpK7",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1j9kXqN1mbP4p8UZmRZX4T4ZLOsXsnFnk",
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
