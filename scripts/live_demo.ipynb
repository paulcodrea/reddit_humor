{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/live_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwhFMk4ZbZeF",
        "outputId": "f915266d-bf15-464c-dbe6-1e098f0edc71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pickle5\n",
        "!pip3 install termcolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFCN2YbnZt8F",
        "outputId": "5ad91810-dd13-4d88-d2b3-cab912643c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## THIS iS A PYTHON SCRIPT TO RUN THE CODE FOR LIVE DEMO\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import pickle5 as pickle\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from termcolor import colored\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou2jlv5ZZ3X2",
        "outputId": "3f14b092-bf9e-412f-f2c4-8b3930cf5db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oSpAlI4CaIOE"
      },
      "outputs": [],
      "source": [
        "class humour_live_demo:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.data_df = pd.DataFrame()\n",
        "        \n",
        "        self.max_length = None\n",
        "        self.corpus_size = None\n",
        "        self.input_vec = []\n",
        "\n",
        "\n",
        "    def pre_process(self, text):\n",
        "        \"\"\"\n",
        "        Pre-processes the input and returns the output. \n",
        "        Removes stopwords, punctuation, and emojis.\n",
        "        \"\"\"\n",
        "        text = re.sub(r'http\\S+', '', text) # remove links\n",
        "        text = re.sub(r'[^\\w\\s]','', text) # remove punctuation\n",
        "\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        text = emoji_pattern.sub(r'', text) # remove emoji\n",
        "        text = ' '.join([word for word in word_tokenize(text) if word not in stopwords]) # remove stopwords\n",
        "        text = text.lower()\n",
        "        ret = word_tokenize(text)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def process_input(self, text):\n",
        "        \"\"\"\n",
        "        Processes the input and returns the output numerical vector.\n",
        "        \"\"\"\n",
        "        input = self.pre_process(text)\n",
        "        input = [word for word in input if word.isalpha()]\n",
        "        input = [word for word in input if not word.startswith(\"http\")]\n",
        "        print(\"Clean input: \", ' '.join(input))\n",
        "\n",
        "        input_numerical = self.tokenizer.texts_to_sequences(input) # Convert to numerical\n",
        "        input_numerical = [item for sublist in input_numerical for item in sublist] # Flatten the list\n",
        "        input_numerical = np.array([input_numerical]) # Convert to numpy array\n",
        "        input_numerical = pad_sequences(input_numerical, maxlen=int(self.max_length)) # Pad the input\n",
        "        input_numerical = np.array(input_numerical, dtype=np.float32)\n",
        "        \n",
        "        self.input_vec = input_numerical # Set the input vector\n",
        "\n",
        "\n",
        "    def process_tf_idf(self, text):\n",
        "        \"\"\"\n",
        "        Processes the tf-idf dataframe and returns the output. \n",
        "        \"\"\"\n",
        "        input = self.pre_process(text)\n",
        "\n",
        "        # Get the tf-idf vector\n",
        "        # Create the word frequency.\n",
        "        word_freq = {}\n",
        "        for word in input:\n",
        "            word_freq[word] = 0\n",
        "        for word in input:\n",
        "            word_freq[word] = word_freq[word] + 1\n",
        "\n",
        "        # Create the tf vector\n",
        "        tf = {}\n",
        "        for word in input:\n",
        "            tf[word] = word_freq[word] / len(input)\n",
        "\n",
        "        # Computing idf\n",
        "        idf = {}\n",
        "        for word in input:\n",
        "            if word in self.tokenizer.keys():\n",
        "                idf[word] = math.log((self.corpus_size + 1)/ (len(self.tokenizer[word]) + 1))\n",
        "            else:\n",
        "                idf[word] = 0\n",
        "\n",
        "        # compute tf-idf\n",
        "        tf_idf = []\n",
        "        for word in input:\n",
        "            tf_idf.append(tf[word] * idf[word])\n",
        "\n",
        "        input_numerical = np.array([tf_idf]) # Convert to numpy array\n",
        "        input_numerical = pad_sequences(input_numerical, maxlen=self.max_length,\n",
        "                                        dtype=np.float32) # Pad the input\n",
        "\n",
        "        self.input_vec = input_numerical\n",
        "\n",
        "\n",
        "    def run_model(self, model_name, dataset_name):\n",
        "        \"\"\"\n",
        "        Runs the model and returns the output\n",
        "        \"\"\"\n",
        "        output = self.model.predict(self.input_vec)\n",
        "        output = np.round(output)\n",
        "        print(\"\\nModel predicts: \", int(output))\n",
        "\n",
        "        self.legend(model_name, dataset_name)\n",
        "        # print(legend)\n",
        "\n",
        "        if int(output) == 0:\n",
        "            print(colored(\"The input is predicted as - not a joke\", \"red\"))\n",
        "        else:\n",
        "            print(colored(\"The input is predicted as - a joke\", \"green\"))\n",
        "\n",
        "\n",
        "    def legend(self, model_name, dataset_name):\n",
        "        \"\"\"\n",
        "        Prints the legend. This is to explain the model to the user.\n",
        "        \"\"\"\n",
        "        final_name = \"\"\n",
        "        # if model contains the word \"tf-idf\"\n",
        "        if \"2a\" in model_name:\n",
        "            final_name = \"LSTM trained on word_id embedding\"\n",
        "        elif \"2b\" in model_name:\n",
        "            final_name = \"LSTM trained on tf-idf embedding\"\n",
        "        elif \"2c\" in model_name:\n",
        "            final_name = \"LSTM trained on word2vec embedding\"\n",
        "        elif \"3\" in model_name:\n",
        "            final_name = \"Random Forest trained on word_id embedding\"\n",
        "        else:\n",
        "            final_name = \"Model not found\"\n",
        "        \n",
        "        if \"1a\" in dataset_name:\n",
        "            final_name = final_name + \" and dataset contains dadjokes, BadJokes (no stopwords and case folding)\"\n",
        "        elif \"1b\" in dataset_name:\n",
        "            final_name = final_name + \" and dataset contains dadjokes, BadJokes (with stopwords and case folding)\"\n",
        "        elif \"2a\" in dataset_name:\n",
        "            final_name = final_name + \" and dataset contains dadjokes, facts (no stopwords and no case folding)\"\n",
        "        elif \"2b\" in dataset_name:\n",
        "            final_name = final_name + \" and dataset contains dadjokes, facts (with stopwords and case folding)\"\n",
        "        \n",
        "        print(colored(final_name, \"red\"))\n",
        "\n",
        "    def run_script(self, input):\n",
        "        \"\"\"\n",
        "        This is a function to run the script.\n",
        "        \"\"\"\n",
        "        print(\"\\n______________________________________________________________\")\n",
        "        for file in os.listdir(self.path):\n",
        "            if file.endswith(\".h5\"):\n",
        "                # THIS IS FOR LSTM, word_id and word2vec-------------------------\n",
        "                if file.startswith(\"2a\") or file.startswith(\"2c\"):\n",
        "                    model_name = file[:2] # parse file name to get first 2 characters from file name\n",
        "                    dataset_name = file[2:-8] # parse file name to get the dataset name\n",
        "\n",
        "                    # 1. Read Model\n",
        "                    try: \n",
        "                        self.model = load_model(self.path + file)\n",
        "                        print(\"\\n\\nModel loaded: \" + file)\n",
        "                    except Exception as e:\n",
        "                        print(e)\n",
        "                        print(\"Model not loaded\")\n",
        "                        continue\n",
        "\n",
        "                    # 2. Read Data \n",
        "                    try:\n",
        "                        self.data_df = pd.read_csv(self.path + model_name \n",
        "                                                          + dataset_name + 'data.csv')\n",
        "                        print(\"Data loaded: \" + model_name + '_data.csv')\n",
        "                    except:\n",
        "                        print(\"No data found\")\n",
        "                        continue\n",
        "\n",
        "                    # 3. Read Pickle file\n",
        "                    try:\n",
        "                        with open(self.path + model_name + dataset_name \n",
        "                                  + 'tokenizer.pickle', 'rb') as handle:\n",
        "                            self.tokenizer = pickle.load(handle)\n",
        "                        print(\"Tokenizer loaded: \" + model_name + '_tokenizer.pickle')\n",
        "                    except:\n",
        "                        print(\"No tokenizer found\")\n",
        "                        continue\n",
        "                    \n",
        "                    # Save Max Length\n",
        "                    self.max_length = int(self.data_df['max_len'][0])\n",
        "\n",
        "                    # 4. Process the input\n",
        "                    self.process_input(ret)\n",
        "                    self.run_model(model_name, dataset_name)\n",
        "\n",
        "                 # THIS IS FOR LSTM tf-idf ------------------------------------\n",
        "                elif file.startswith(\"2b-\") and file.endswith(\"model.h5\"):\n",
        "                    model_name = file[:2]\n",
        "                    dataset_name = file[2:-8] # parse file name to get the dataset name\n",
        "\n",
        "\n",
        "                    # 1. Read Model\n",
        "                    try:\n",
        "                        self.model = load_model(self.path + file)\n",
        "                        print(\"\\n\\nModel loaded: \" + file)\n",
        "                    except Exception as e:\n",
        "                        print(e)\n",
        "                        print(\"Model not loaded\")\n",
        "                        continue\n",
        "\n",
        "                    # 2. Read Data\n",
        "                    try:\n",
        "                        self.data_df = pd.read_csv(self.path + model_name \n",
        "                                                          + dataset_name + 'data.csv')\n",
        "                        print(\"Data loaded: \" + model_name + '_data.csv')\n",
        "                    except:\n",
        "                        print(\"No data found\")\n",
        "                        continue\n",
        "\n",
        "                    # 3. Read Pickle file\n",
        "                    try:\n",
        "                        with open(self.path + model_name + dataset_name \n",
        "                                  + 'tokenizer.pickle', 'rb') as handle:\n",
        "                            self.tokenizer = pickle.load(handle)\n",
        "                        print(\"Tokenizer loaded: \" + model_name + dataset_name\n",
        "                              + 'tokenizer.pickle')\n",
        "                    except:\n",
        "                        print(\"No tokenizer found\")\n",
        "                        continue\n",
        "\n",
        "                    # Save Max Length\n",
        "                    self.max_length = int(self.data_df['max_len'][0])\n",
        "                    self.corpus_size = int(self.data_df['corpus_size'][0])\n",
        "\n",
        "                    # 4. Process the input\n",
        "                    self.process_tf_idf(ret)\n",
        "                    self.run_model(model_name, dataset_name)\n",
        "\n",
        "\n",
        "            # THIS IS FOR RANDOM FOREST ----------------------------------------\n",
        "            elif file.startswith(\"3-\") and file.endswith(\"model.pickle\"):\n",
        "                model_name = file[:1]\n",
        "                dataset_name = file[1:-12] # parse file name to get the dataset name\n",
        "\n",
        "                # 1. Read Model\n",
        "                try: \n",
        "                    with open(self.path + model_name + dataset_name \n",
        "                              + 'model.pickle', 'rb') as handle:\n",
        "                        self.model = pickle.load(handle)\n",
        "                    print(\"\\n\\nModel loaded: \" + file)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                    print(\"Model not loaded\")\n",
        "                    continue\n",
        "\n",
        "                # 2. Read Data \n",
        "                try:\n",
        "                    self.data_df = pd.read_csv(self.path + model_name \n",
        "                                                      + dataset_name + 'data.csv')\n",
        "                    print(\"Data loaded: \" + model_name + '_data.csv')\n",
        "                except:\n",
        "                    print(\"No data found\")\n",
        "                    continue\n",
        "\n",
        "                # 3. Read Pickle file\n",
        "                try:\n",
        "                    with open(self.path + model_name + dataset_name \n",
        "                              + 'tokenizer.pickle', 'rb') as handle:\n",
        "                        self.tokenizer = pickle.load(handle)\n",
        "                    print(\"Tokenizer loaded: \" + model_name + '_tokenizer.pickle')\n",
        "                except:\n",
        "                    print(\"No tokenizer found\")\n",
        "                    continue\n",
        "                \n",
        "                # Save Max Length\n",
        "                self.max_length = int(self.data_df['max_len'][0])\n",
        "\n",
        "                # 4. Process the input\n",
        "                self.process_input(ret)\n",
        "                self.run_model(model_name, dataset_name)\n",
        "\n",
        "        print(\"______________________________________________________________\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhmGLHD9h3Ww"
      },
      "source": [
        "### Add the path to the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "anyBIOrSaVjv"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/NLU_Humour-detection/COMP34812/best_models/\"\n",
        "live_session = humour_live_demo(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9gXP3xiEr0"
      },
      "source": [
        "### Get input from the user and predict if the input is funny or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvi93dwEaYz6",
        "outputId": "330199df-bb1b-41f8-844b-fcf97f08bce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: Helvetica and Times New Roman walk into a bar. “Get out of here!” shouts the bartender. “We don’t serve your type.”\n",
            "\n",
            "______________________________________________________________\n",
            "\n",
            "\n",
            "Model loaded: 2a-dataset_2a_model.h5\n",
            "Data loaded: 2a_data.csv\n",
            "Tokenizer loaded: 2a_tokenizer.pickle\n",
            "Clean input:  helvetica times new roman walk bar get shouts bartender we dont serve type\n",
            "\n",
            "Model predicts:  1\n",
            "\u001b[31mLSTM trained on word_id embedding and dataset contains dadjokes, facts (no stopwords and no case folding)\u001b[0m\n",
            "\u001b[32mThe input is predicted as - a joke\u001b[0m\n",
            "\n",
            "\n",
            "Model loaded: 2c-dataset_2b_model.h5\n",
            "Data loaded: 2c_data.csv\n",
            "Tokenizer loaded: 2c_tokenizer.pickle\n",
            "Clean input:  helvetica times new roman walk bar get shouts bartender we dont serve type\n",
            "\n",
            "Model predicts:  0\n",
            "\u001b[31mLSTM trained on word2vec embedding and dataset contains dadjokes, facts (with stopwords and no case folding)\u001b[0m\n",
            "\u001b[31mThe input is predicted as - not a joke\u001b[0m\n",
            "\n",
            "\n",
            "Model loaded: 3-dataset_2a_model.pickle\n",
            "Data loaded: 3_data.csv\n",
            "Tokenizer loaded: 3_tokenizer.pickle\n",
            "Clean input:  helvetica times new roman walk bar get shouts bartender we dont serve type\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model predicts:  1\n",
            "\u001b[31mRandom Forest trained on word_id embedding and dataset contains dadjokes, facts (no stopwords and no case folding)\u001b[0m\n",
            "\u001b[32mThe input is predicted as - a joke\u001b[0m\n",
            "\n",
            "\n",
            "Model loaded: 2b-dataset_2a_model.h5\n",
            "Data loaded: 2b_data.csv\n",
            "Tokenizer loaded: 2b-dataset_2a_tokenizer.pickle\n",
            "\n",
            "Model predicts:  0\n",
            "\u001b[31mLSTM trained on tf-idf embedding and dataset contains dadjokes, facts (no stopwords and no case folding)\u001b[0m\n",
            "\u001b[31mThe input is predicted as - not a joke\u001b[0m\n",
            "______________________________________________________________\n",
            "\n",
            "\n",
            "Enter a sentence: exit\n",
            "End of model...\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    ret = input(\"Enter a sentence: \") # Read input from user\n",
        "\n",
        "    # Check if the user wants to exit\n",
        "    if ret == \"exit\":\n",
        "      print(\"End of model...\")\n",
        "      break\n",
        "\n",
        "    live_session.run_script(ret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74xQpQ_9f2mb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN8tVQPnkHk9eI+W2QESOub",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1uLDNHphYfUpa1t41UvQvASihwINHKPyC",
      "name": "live_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
