{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b_tf_idf.ipynb",
      "provenance": [],
      "mount_file_id": "1n1NVwTzbJcVM7E6J96l2_06n41KRQISl",
      "authorship_tag": "ABX9TyPsxCUSHY1OCoe5Q/s2nA2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/2b_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a tf-idf embedding vector to represent every joke in the corpus"
      ],
      "metadata": {
        "id": "_-KO2OLEQLwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk import word_tokenize, RegexpTokenizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYGl01tRxB7",
        "outputId": "ce4ce9be-1afa-424e-cdef-907730c7ce86"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from the csv:"
      ],
      "metadata": {
        "id": "OdXW-7UDQYL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSYc96Ad4shW",
        "outputId": "e3f1973d-2deb-426e-9e03-d0556a259806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfuly read!\n",
            "This is the first line from the data file: \n",
            "    Unnamed: 0      joke          tokens  token_count  funny clean_text\n",
            "0           0  RIP Cob.  ['RIP', 'Cob']            2      0   rip cob \n"
          ]
        }
      ],
      "source": [
        "path = 'drive/MyDrive/Humour_Detection_Dataset/final_jokes(1200).csv'\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "print(\"Data successfuly read!\")\n",
        "\n",
        "print(\"This is the first line from the data file: \\n\", data.head(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "6iP56LPWQebX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions below enable the data from the csv to be transformed in variables used to compute the tf and idf."
      ],
      "metadata": {
        "id": "fYtRZnJjSDiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenise_clean_text(data_set):\n",
        "  \"\"\"\n",
        "  Tokenises the data from the CSV file.\n",
        "  \"\"\"\n",
        "  arr = []\n",
        "  for line in data_set['clean_text']:\n",
        "    arr.append(line.split(' '))\n",
        "  arr = [x[:(len(x) - 1)] for x in arr]\n",
        "  return arr\n",
        "\n",
        "def return_vocab(document) -> list:\n",
        "  \"\"\"\n",
        "  Retuns the vocabulary of a document.\n",
        "  \"\"\"\n",
        "  computed_vocab = []\n",
        "  for line in document:\n",
        "    for word in line:\n",
        "      computed_vocab.append(word)\n",
        "  computed_vocab = list(dict.fromkeys(computed_vocab))\n",
        "  return computed_vocab\n",
        "\n",
        "def term_frequency_corpus(vocabulary, list_jokes):\n",
        "  \"\"\"\n",
        "  Returns the a dictionary where the keys are the vocabulary words and the values\n",
        "  are the frequency of the word in the whole corpus.\n",
        "  \"\"\"\n",
        "  vocab_freq = {}\n",
        "  for word in vocabulary:\n",
        "    vocab_freq[word] = 0\n",
        "  for joke in list_jokes:\n",
        "    for token in joke:\n",
        "      vocab_freq[token] = vocab_freq[token] + 1\n",
        "  return vocab_freq\n",
        "\n",
        "def term_frequency(vocab: dict, jokes: list):\n",
        "  \"\"\"\n",
        "  Returns a dictionary where the keys are the vocabulary words and the values are\n",
        "  a list that represents the frequency of the word in each joke divided by the \n",
        "  length of the joke.\n",
        "  \"\"\"\n",
        "  dict_term_freq = {}\n",
        "  for word in vocab.keys():\n",
        "    dict_term_freq[word] = []\n",
        "  for word in vocab.keys():\n",
        "    for joke in jokes:\n",
        "      freq = 0\n",
        "      for word_index in range(len(joke)):\n",
        "        if word == joke[word_index]:\n",
        "          freq = freq + 1\n",
        "      dict_term_freq[word].append(freq / len(joke))\n",
        "  return dict_term_freq\n",
        "\n",
        "def document_frequency(vocabulary: dict, jokes: list):\n",
        "  \"\"\"\n",
        "  Returns a dictionary where the keys are all words in the vocabulary and the \n",
        "  values the list of jokes the word appears in.\n",
        "  \"\"\"\n",
        "  doc_freq = {}\n",
        "  for word in vocabulary.keys():\n",
        "    doc_freq[word] = []\n",
        "    for joke_index in range(len(jokes)):\n",
        "      if word in jokes[joke_index]:\n",
        "        doc_freq[word].append(joke_index)\n",
        "  return doc_freq\n",
        "\n"
      ],
      "metadata": {
        "id": "7QM9NSON74V3"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "list_jokes_tokenized = tokenise_clean_text(data)\n",
        "vocabulary = return_vocab(list_jokes_tokenized)\n",
        "print(\"The vocabulary has been created!\")\n",
        "print(\"This is the len of the vocabulary:\\n \", len(vocabulary))\n",
        "\n",
        "print(\"This is the first token from the corpus, pre-processed: \\n\", vocabulary[0])\n",
        "print(\"This is the first item from the tokenised list of jokes:\\n\", list_jokes_tokenized[0])\n",
        "print(\"This is how many jokes the data set has:\\n\",len(list_jokes_tokenized))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXR2UQW8-y8",
        "outputId": "389ac101-6ad9-4b31-da3d-9155cdb8af3f"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary has been created!\n",
            "This is the len of the vocabulary:\n",
            "  4473\n",
            "This is the first token from the corpus, pre-processed: \n",
            " rip\n",
            "This is the first item from the tokenised list of jokes:\n",
            " ['rip', 'cob']\n",
            "This is how many jokes the data set has:\n",
            " 1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "at this point we have:\n",
        "\n",
        "vocabulary -> all tokens that appear in the document\n",
        "\n",
        "list_jokes_tokenized -> all jokes represented by a list of words. To be used to generate embeddings!"
      ],
      "metadata": {
        "id": "tLpplMybWPI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency (TF) calculation"
      ],
      "metadata": {
        "id": "4K1aN4gdRTYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TF for each word in the vocabulary:\n",
        "\n",
        "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
      ],
      "metadata": {
        "id": "rnrgkDZYRKeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the frequency of each word from the vocabulary in the corpus \n",
        "vocab_freq = term_frequency_corpus(vocabulary, list_jokes_tokenized)\n",
        "print(\"The vocabulary with the associated frequency for each word has been generated!\")\n",
        "print(\"This is how many times the word 'joke' is in our corpus\\n\", vocab_freq[\"joke\"])\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJigOi636i2r",
        "outputId": "23098fc4-c2e9-4709-aa84-8c6c58e40d03"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary with the associated frequency for each word has been generated!\n",
            "This is how many times the word 'joke' is in our corpus\n",
            " 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(vocab_freq)\n",
        "print(\"This is the length of our vocabulary:\\n\", vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9V-ZLr_2QSY",
        "outputId": "1f16fbfc-a87c-46d8-dd08-e0491098cf14"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the length of our vocabulary:\n",
            " 4473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate tf number of times a word appears in a doc / total number of words in the doc\n",
        "tf = term_frequency(vocab_freq, list_jokes_tokenized)\n",
        "print(\"Term frequency matrix/dictionary was generated!\")\n",
        "print(\"This is the term frequency vector for word 'joke':\\n\", tf[\"joke\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB4hHCsXIpTM",
        "outputId": "11638def-f9c1-447c-c3ff-15b84e7139f7"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term frequency matrix/dictionary was generated!\n",
            "This is the term frequency vector for word 'joke':\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018518518518518517, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.061855670103092786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027777777777777776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058823529411764705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.038461538461538464, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15789473684210525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.02702702702702703, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010101010101010102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IDF calculation"
      ],
      "metadata": {
        "id": "RwFZQ0KcRcx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating IDF for each word:\n",
        "\n",
        "The log of the number of documents divided by the number of documents that contain the word w. The IDF of a word is computed once for all documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "wUHwDnEYRewS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "# create a list of all the docs in which the vocab words occur -> to be used in idf\n",
        "dict_document_freq = document_frequency(vocab_freq, list_jokes_tokenized)\n",
        "print(\"A list of all documents in which the words from our vocab appear in have been generated!\")\n",
        "print(\"The list of all documents that contain the word 'Joke' looks like this:\\n\",dict_document_freq[\"joke\"] )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLOBMkJHI1VD",
        "outputId": "be7e612b-d0d6-4f90-cb5a-adfc4dd4e8f6"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A list of all documents in which the words from our vocab appear in have been generated!\n",
            "The list of all documents that contain the word 'Joke' looks like this:\n",
            " [70, 73, 182, 235, 275, 282, 288, 295, 348, 365, 377, 386, 392, 395, 463, 468, 484, 604, 651, 662, 713, 721, 739, 777, 782, 797, 819, 859, 876, 914, 928, 931, 936, 946, 951, 956, 971, 988, 1014, 1029, 1095, 1096, 1097, 1121, 1133, 1158, 1189]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "idf = {}\n",
        "for word in dict_document_freq.keys():\n",
        "  idf[word] = math.log(len(list_jokes_tokenized)/len(dict_document_freq[word]))\n",
        "print(\"A dictionary with each word's tf-idf values has been generated!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Xv4YplZbQD",
        "outputId": "16541d00-3df5-416b-dd0b-a92188db8443"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A dictionary with each word's tf-idf values has been generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF calculation"
      ],
      "metadata": {
        "id": "Wwgfz6_ZRn8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is simply the TF multiplied by IDF.\n",
        "\n",
        "The output will be a list of tf-idf values (word x joke)"
      ],
      "metadata": {
        "id": "nha9w2c4RqHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_tf_idf = {}\n",
        "\n",
        "for word in vocab_freq.keys():\n",
        "  dict_tf_idf[word] = []\n",
        "  for tf_index in range(len(tf[word])):\n",
        "    dict_tf_idf[word].append(tf[word][tf_index] * idf[word])\n",
        "\n",
        "print(\"The tf-idf values have beed generated! \")\n",
        "print(\"this is how the tf-idf vector looks like for the word 'joke':\\n\",dict_tf_idf[\"joke\"])\n",
        "print(\"This is the length of each tf-idf vector. Each value represent's the word's tf-idf for each sentence:\\n\", len(dict_tf_idf[\"joke\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apciYnoNRaCw",
        "outputId": "83dd3ace-5ffc-42dc-cb35-f5789aba6489"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tf-idf values have beed generated! \n",
            "this is how the tf-idf vector looks like for the word 'joke':\n",
            " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6479858468132067, 0.0, 0.0, 0.8099823085165083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35999213711844813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8099823085165083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4628470334380047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12959716936264135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26999410283883607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6199646170330166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12959716936264135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16199646170330168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21599528227106887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29453902127873033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5399882056776721, 0.0, 0.0, 0.6479858468132067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.059998689519741355, 0.0, 0.0, 0.0, 0.0, 0.6479858468132067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20040799385975464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29453902127873033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32399292340660335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08999803427961203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32399292340660335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29453902127873033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40499115425825416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0799764113553443, 0.0, 0.0, 0.0, 0.0, 0.35999213711844813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6479858468132067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5399882056776721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4628470334380047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1905840725921196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26999410283883607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29453902127873033, 0.0, 0.0, 0.1246126628486936, 0.0, 0.0, 0.0, 0.0, 0.8099823085165083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0799764113553443, 0.0, 0.0, 0.0, 0.0, 0.2492253256973872, 0.0, 0.0, 0.0, 0.0, 0.4628470334380047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12959716936264135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35999213711844813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5115677737998999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40499115425825416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40499115425825416, 0.08756565497475766, 1.6199646170330166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03272655791985892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26999410283883607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21599528227106887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32399292340660335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "This is the length of each tf-idf vector. Each value represent's the word's tf-idf for each sentence:\n",
            " 1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representing each joke with tf-idf values"
      ],
      "metadata": {
        "id": "0RzQ3Hn9Ufyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing each joke by an embedding vector."
      ],
      "metadata": {
        "id": "0myX5LnTbBn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_as_tf_idf = []\n",
        "max_joke_size = 0\n",
        "\n",
        "\n",
        "for joke_index in range(len(list_jokes_tokenized)):\n",
        "  joke = list_jokes_tokenized[joke_index]\n",
        "  tf_idf_list = []\n",
        "  for word in joke:\n",
        "    tf_idf_list.append(dict_tf_idf[word][joke_index])\n",
        "  tf_idf = np.asarray(tf_idf_list)\n",
        "  jokes_as_tf_idf.append(tf_idf)\n",
        "\n",
        "  # calculate max_joke size to use in the padding step\n",
        "  if len(joke) > max_joke_size:\n",
        "    max_joke_size = len(tf_idf_list)\n",
        "    \n",
        "jokes_as_tf_idf = np.asarray(jokes_as_tf_idf)\n",
        "\n",
        "print(\"The jokes have been embedded by their tf-idf values!\")\n",
        "print(\"This is the max joke length for our corpus: \", max_joke_size)\n",
        "print(\"This is how the first joke looks like before the embedding step: \\n\", list_jokes_tokenized[0])\n",
        "print(\"This is how the tf-idf embedding of the first joke looks like:\\n\",jokes_as_tf_idf[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_CQ7CirUe2o",
        "outputId": "d2b29b93-4bf4-4dd7-e2cf-22fc591101c7"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The jokes have been embedded by their tf-idf values!\n",
            "This is the max joke length for our corpus:  134\n",
            "This is how the first joke looks like before the embedding step: \n",
            " ['rip', 'cob']\n",
            "This is how the tf-idf embedding of the first joke looks like:\n",
            " [3.54503842 3.54503842]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_as_tf_idf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyU0rwhrdOSS",
        "outputId": "5f4998e1-6798-4607-be4d-a920ff6d72b8"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200,)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split train - test data"
      ],
      "metadata": {
        "id": "qjk2HGN6guhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad the jokes. - at the end"
      ],
      "metadata": {
        "id": "I9hbs2j4zTMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_arr = np.zeros((len(jokes_as_tf_idf), max_joke_size))\n",
        "old_arr = jokes_as_tf_idf\n",
        "for idx, joke in enumerate(jokes_as_tf_idf):\n",
        "  len_joke = len(joke)\n",
        "  joke_x = jokes_as_tf_idf[idx]\n",
        "  new_arr[idx] = np.append(joke, [0] * (max_joke_size - len_joke))\n",
        "\n",
        "print(new_arr.shape)\n",
        "jokes_as_tf_idf = new_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0z3NCHzvEVY",
        "outputId": "59e84ef5-0ae6-4f48-dc58-21f1380cec92"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 134)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_train_test_data(jokes_list, corpus):\n",
        "  train_max_index = int(len(jokes_list) * 0.8)\n",
        "  X_train = np.zeros(shape=(train_max_index, max_joke_size))\n",
        "  X_test = np.zeros(shape=(len(jokes_list) - train_max_index, max_joke_size))\n",
        "  index_returned_data = 0\n",
        "  for index in range(0, train_max_index):\n",
        "    X_train[index_returned_data] = jokes_as_tf_idf[index]\n",
        "  y_train = corpus[\"funny\"][:train_max_index]\n",
        "  index_returned_data = 0\n",
        "  for index in range(train_max_index, len(jokes_as_tf_idf)):\n",
        "    X_test[index_returned_data] = jokes_as_tf_idf[index]\n",
        "  y_test = corpus[\"funny\"][train_max_index:]\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "nvXaf66eiwnR"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_test_data(jokes_as_tf_idf, testing_index_list, data)\n",
        "X_train, X_test, y_train, y_test = generate_train_test_data(jokes_as_tf_idf, data)"
      ],
      "metadata": {
        "id": "qykm6HeWoGmm"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying it on the LSTM"
      ],
      "metadata": {
        "id": "-Wjbdq4sir4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHfVRE5IiuRp",
        "outputId": "4ae0c1bd-b6f4-4aca-c111-8c96cb268b16"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 3, \n",
        "    \"batch_size\": 70,\n",
        "    \"test_p\": 0.2,\n",
        "    \"val_p\": 0.1,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'softmax',\n",
        "}"
      ],
      "metadata": {
        "id": "vzIGRYMWi15r"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_model(max_length, X_train, X_test, y_train, y_test, vocabulary_size):\n",
        "  \"\"\"\n",
        "  Splits the data into train and validation sets.\n",
        "\n",
        "  Constructs the LSTM model.\n",
        "  \"\"\"\n",
        "\n",
        "  # print(\"X shape: \", X.shape)\n",
        "  # print(\"y shape: \", y.shape)\n",
        "\n",
        "  # split the data into train and validation sets and make them random\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config['test_p'], random_state=42)\n",
        "\n",
        "  print(\"X_train:\", len(X_train))\n",
        "  print(\"X_test:\", len(X_test))\n",
        "  print(\"y_train:\", len(y_train))\n",
        "  print(\"y_test:\", len(y_test))\n",
        "\n",
        "  print(X_train.shape)\n",
        "  print(y_train.shape)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocabulary_size, output_dim=max_length, input_length=int(X_train.shape[1])))\n",
        "  model.add(Dropout(config['Dropout_layer'][0]))\n",
        "  model.add(LSTM(config['LSTM_layer'][1], activation=config['activation']))\n",
        "  model.add(Dropout(config['Dropout_layer'][1]))\n",
        "  model.add(Dense(units=1, activation=config['activation']))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto', validation_split=config['val_p'])\n",
        "\n",
        "  # Evaluate the model\n",
        "  scores = model.evaluate(X_test, y_test)\n",
        "  print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "  # Print Precision and Recall\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred = np.round(y_pred)\n",
        "  print(\"Precision: %.2f%%\" % (precision_score(y_test, y_pred) * 100))\n",
        "  print(\"Recall: %.2f%%\" % (recall_score(y_test, y_pred) * 100))\n",
        "  print(\"F1-Score: %.2f%%\" % (f1_score(y_test, y_pred) * 100))"
      ],
      "metadata": {
        "id": "A583qWMsi36P"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model(max_joke_size, X_train, X_test, y_train, y_test, len(vocab_freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-yY7GlWi6dW",
        "outputId": "80164e90-88fc-4b2b-c4f9-5ac5336a8c3a"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: 960\n",
            "X_test: 240\n",
            "y_train: 960\n",
            "y_test: 240\n",
            "(960, 134)\n",
            "(960,)\n",
            "Epoch 1/3\n",
            "13/13 [==============================] - 7s 323ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6928 - val_accuracy: 0.4062\n",
            "Epoch 2/3\n",
            "13/13 [==============================] - 4s 292ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.4062\n",
            "Epoch 3/3\n",
            "13/13 [==============================] - 4s 308ms/step - loss: 0.6930 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.4062\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.6932 - accuracy: 0.5333\n",
            "Accuracy: 53.33%\n",
            "Precision: 53.33%\n",
            "Recall: 100.00%\n",
            "F1-Score: 69.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SivgGfRGqYkY"
      },
      "execution_count": 173,
      "outputs": []
    }
  ]
}