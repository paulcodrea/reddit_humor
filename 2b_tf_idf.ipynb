{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/2b_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-KO2OLEQLwz"
      },
      "source": [
        "# LSTM with a tf-idf embedding vector "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZBXpowG1Dx"
      },
      "source": [
        "## Imports + Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\paulc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vzIGRYMWi15r"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 3, \n",
        "    \"batch_size\": 70,\n",
        "    \"train_p\": 0.7,\n",
        "    \"test_p\": 0.2,\n",
        "    \"val_p\": 0.1,\n",
        "    \"activation\": 'sigmoid',\n",
        "    ##################### SAVE FOR LIVE DEMO #############################\n",
        "    \"model_path\": './model/2b-dataset_1a_model.h5',\n",
        "    \"tokenizer_path\": './model/2b-dataset_1a_tokenizer.pickle',\n",
        "    \"data_path\": \"model/2b-dataset_1a_data.csv\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FECe7TUlH1O_",
        "outputId": "f14fd5bf-597a-41c7-e3c0-49d4c0801a9b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdXW-7UDQYL9"
      },
      "source": [
        "# Methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ixXhMeXke24M"
      },
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "    self.data = pd.DataFrame()\n",
        "    self.max_joke_size = 0\n",
        "    self.vocabulary_size = 0\n",
        "    self.list_jokes_tokenized = []\n",
        "    \n",
        "    self.tokenizer = Tokenizer(num_words=None, split=' ')\n",
        "    self.model = None\n",
        "\n",
        "  def read_dataset(self):\n",
        "    self.data = pd.read_csv(self.path)\n",
        "    print(\"Data successfuly read!\")\n",
        "    # print(\"This is the first line from the data file: \\n\", self.data.head(1))\n",
        "\n",
        "  def tokenise_clean_text(self):\n",
        "    \"\"\"\n",
        "    Tokenises the data from the CSV file.\n",
        "    Since the CSV has a \"clean_text\" column, the only required pre-processing\n",
        "    is splitting it from white spaces.\n",
        "    \"\"\"\n",
        "    arr = []\n",
        "    for line in self.data['clean_text']:\n",
        "      arr.append(line.split(' '))\n",
        "    self.list_jokes_tokenized = [x[:(len(x) - 1)] for x in arr] # remove '' at the end\n",
        "    print('Tokenisation is complete!')\n",
        "    # print(\"This is the first item from the tokenised list of jokes:\\n\", self.list_jokes_tokenized[0])\n",
        "    # print(\"This is how many jokes the data set has:\\n\",len(self.list_jokes_tokenized))\n",
        "\n",
        "  def return_vocab(self) -> list:\n",
        "    \"\"\"\n",
        "    Retuns the vocabulary of the document.\n",
        "    \"\"\"\n",
        "    computed_vocab = []\n",
        "    for line in self.list_jokes_tokenized:\n",
        "      for word in line:\n",
        "        computed_vocab.append(word)\n",
        "    computed_vocab = list(dict.fromkeys(computed_vocab))\n",
        "\n",
        "    print(\"The vocabulary has been created!\")\n",
        "    # print(\"This is the len of the vocabulary:\\n \", len(computed_vocab))\n",
        "    # print(\"This is the first token from the corpus, pre-processed: \\n\", computed_vocab[0])\n",
        "\n",
        "    self.vocabulary_size = len(computed_vocab)\n",
        "    return computed_vocab\n",
        "\n",
        "  def return_terms_frequencies(self, vocabulary):\n",
        "    \"\"\"\n",
        "    Returns the term frequency dictionary. The keys are the vocabulary words and\n",
        "    the values are the frequency of the words in the whole corpus.\n",
        "    \"\"\"\n",
        "    vocab_freq = {}\n",
        "    for word in vocabulary:\n",
        "      vocab_freq[word] = 0\n",
        "    for joke in self.list_jokes_tokenized:\n",
        "      for token in joke:\n",
        "        vocab_freq[token] = vocab_freq[token] + 1\n",
        "    \n",
        "    print(\"The vocabulary with the associated frequency for each word has been generated!\")\n",
        "    # print(\"This is how many times the word 'joke' is in our corpus\\n\", vocab_freq[\"joke\"])\n",
        "    \n",
        "    return vocab_freq\n",
        "\n",
        "  def generate_tf(self, vocab: dict):\n",
        "    \"\"\"\n",
        "    Returns a term frequency dictionary. The keys are the vocabulary words and \n",
        "    the values are a list that represents the frequency of the word in each joke\n",
        "    divided by the length of the joke.\n",
        "    \"\"\"\n",
        "    dict_term_freq = {}\n",
        "    for word in vocab.keys():\n",
        "      dict_term_freq[word] = []\n",
        "    for word in vocab.keys():\n",
        "      for joke in self.list_jokes_tokenized:\n",
        "        word_freq = 0\n",
        "        for word_index in range(len(joke)):\n",
        "          if word == joke[word_index]:\n",
        "            word_freq = word_freq + 1\n",
        "        dict_term_freq[word].append(word_freq / len(joke))\n",
        "\n",
        "    print(\"Term frequency matrix/dictionary was generated!\")\n",
        "    # print(\"This is the term frequency vector for word 'joke':\\n\", dict_term_freq[\"joke\"])\n",
        "\n",
        "    return dict_term_freq\n",
        "\n",
        "  def document_frequency(self, vocabulary: dict):\n",
        "    \"\"\"\n",
        "    Returns a document frquency dictionary. The keys are all words in the \n",
        "    vocabulary and the values are a list of jokes the word appears in.\n",
        "    \"\"\"\n",
        "    doc_freq = {}\n",
        "    for word in vocabulary.keys():\n",
        "      doc_freq[word] = []\n",
        "      for joke_index in range(len(self.list_jokes_tokenized)):\n",
        "        if word in self.list_jokes_tokenized[joke_index]:\n",
        "          doc_freq[word].append(joke_index)\n",
        "\n",
        "    print(\"A list of all documents in which the words from our vocab appear in has been generated!\")\n",
        "    # print(\"The list of all documents that contain the word 'Joke' looks like this:\\n\",doc_freq[\"joke\"] )\n",
        "\n",
        "    return doc_freq\n",
        "\n",
        "  def generate_idf(self, document_frequencies):\n",
        "    \"\"\"\n",
        "    Returns an inverse document frequency dictionary. The keys are the vocabulary\n",
        "    words and the values are the corresponding idfs.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    for word in document_frequencies.keys():\n",
        "      idf[word] = math.log(len(self.list_jokes_tokenized)/len(document_frequencies[word]))\n",
        "\n",
        "    print(\"A dictionary with each word's tf-idf values has been generated!\")\n",
        "\n",
        "    return idf\n",
        "\n",
        "  def generate_tf_idf(self, vocab_frequencies, tf, idf):\n",
        "    \"\"\"\n",
        "    Returns a tf-idf dictionary. The keys are the vocabulary words and the \n",
        "    values are the corresponding tf-idfs.\n",
        "    \"\"\"\n",
        "    dict_tf_idf = {}\n",
        "\n",
        "    for word in vocab_frequencies.keys():\n",
        "      dict_tf_idf[word] = []\n",
        "      for tf_index in range(len(tf[word])):\n",
        "        dict_tf_idf[word].append(tf[word][tf_index] * idf[word])\n",
        "\n",
        "    print(\"The tf-idf values have beed generated!\")\n",
        "    # print(\"this is how the tf-idf vector looks like for the word 'joke':\\n\",dict_tf_idf[\"joke\"])\n",
        "    # print(\"This is the length of each tf-idf vector. Each value represent's the word's tf-idf for each sentence:\\n\", len(dict_tf_idf[\"joke\"]))\n",
        "    \n",
        "    return dict_tf_idf\n",
        "\n",
        "  def joke_as_tf_idf(self, dict_tf_idf):\n",
        "    \"\"\"\n",
        "    Returns a list of jokes represented by the words' tf-idf values.\n",
        "    \"\"\"\n",
        "    jokes_as_tf_idf = []\n",
        "\n",
        "    for joke_index in range(len(self.list_jokes_tokenized)):\n",
        "      joke = self.list_jokes_tokenized[joke_index]\n",
        "      tf_idf_list = []\n",
        "      for word in joke:\n",
        "        tf_idf_list.append(dict_tf_idf[word][joke_index])\n",
        "      tf_idf = np.asarray(tf_idf_list)\n",
        "      jokes_as_tf_idf.append(tf_idf)\n",
        "\n",
        "      # calculate max_joke size to use in the padding step\n",
        "      if len(joke) > self.max_joke_size:\n",
        "        self.max_joke_size = len(tf_idf_list)\n",
        "        \n",
        "    jokes_as_tf_idf = np.asarray(jokes_as_tf_idf)\n",
        "\n",
        "    print(\"The jokes have been embedded by their tf-idf values!\")\n",
        "    print(\"This is the max joke length for our corpus: \", self.max_joke_size)\n",
        "    # print(\"This is how the first joke looks like before the embedding step: \\n\", self.list_jokes_tokenized[0])\n",
        "    # print(\"This is how the tf-idf embedding of the first joke looks like:\\n\",jokes_as_tf_idf[0])\n",
        "\n",
        "    return jokes_as_tf_idf\n",
        "\n",
        "  def pad_jokes(self, jokes_as_tf_idf_np):\n",
        "    \"\"\"\n",
        "    Returns the input padded to the max_length of the corpus. The padding will\n",
        "    be at the beginning of the input, and represented by the value 0.\n",
        "    \"\"\"\n",
        "    new_arr = np.zeros((len(jokes_as_tf_idf_np), self.max_joke_size))\n",
        "    for idx, joke in enumerate(jokes_as_tf_idf_np):\n",
        "      len_joke = len(joke)\n",
        "      joke_x = jokes_as_tf_idf_np[idx]\n",
        "      new_arr[idx] = np.append([0] * (self.max_joke_size - len_joke), joke)\n",
        "\n",
        "    print(\"The jokes (input) have been padded to the same size, \", self.max_joke_size)\n",
        "    print(\"This is the shape of the input:\\n\", new_arr.shape)\n",
        "\n",
        "    return new_arr\n",
        "\n",
        "  def generate_train_test_data(self, jokes_list):\n",
        "    \"\"\"\n",
        "    Returns the training and testing data. The data has been shuffled in the\n",
        "    pre-processing step, so the function simply returns 70/10/20 training/validation/testing\n",
        "    data.\n",
        "    \"\"\"\n",
        "    train_max_index = int(len(jokes_list) * config['train_p'])\n",
        "    X_train = np.zeros(shape=(train_max_index, self.max_joke_size))\n",
        "    X_test = np.zeros(shape=(len(jokes_list) - train_max_index, self.max_joke_size))\n",
        "    index_returned_data = 0\n",
        "    \n",
        "    # training data\n",
        "    for index in range(0, train_max_index):\n",
        "      X_train[index_returned_data] = jokes_list[index]\n",
        "      index_returned_data = index_returned_data + 1\n",
        "    y_train = self.data[\"funny\"][:train_max_index]\n",
        "    # testing data\n",
        "    index_returned_data = 0\n",
        "    for index in range(train_max_index, len(jokes_list)):\n",
        "      X_test[index_returned_data] = jokes_list[index]\n",
        "      index_returned_data = index_returned_data + 1\n",
        "    y_test = self.data[\"funny\"][train_max_index:]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "  def LSTM_model(self, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Constructs and evaluates the LSTM model.\n",
        "    \"\"\"\n",
        "    print(\"X_train:\", len(X_train))\n",
        "    print(\"X_test:\", len(X_test))\n",
        "    print(\"y_train:\", len(y_train))\n",
        "    print(\"y_test:\", len(y_test))\n",
        "\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Embedding(input_dim=self.vocabulary_size, output_dim=self.max_joke_size, input_length=int(X_train.shape[1])))\n",
        "    self.model.add(LSTM(50, return_sequences=True))\n",
        "    self.model.add(LSTM(10))\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(units=1, activation=config['activation']))\n",
        "    self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    self.model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto', validation_split=config['val_p'])\n",
        "    \n",
        "    print(\"The model completed the training step!\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = self.model.evaluate(X_test, y_test)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "    # Print Precision and Recall\n",
        "    y_pred = self.model.predict(X_test)\n",
        "    y_pred = np.round(y_pred)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred) * 100\n",
        "    recall = recall_score(y_test, y_pred) * 100\n",
        "    f1 = f1_score(y_test, y_pred) * 100\n",
        "    print(\"Precision: %.2f%%\" % (precision))\n",
        "    print(\"Recall: %.2f%%\" % (recall))\n",
        "    print(\"F1-Score: %.2f%%\" % (f1))\n",
        "\n",
        "    self.save_data(scores[1], precision, recall, f1)\n",
        "\n",
        "\n",
        "  def save_data(self, accuracy, precision, recall, f1):\n",
        "    \"\"\"\n",
        "    Saves the data.\n",
        "    \"\"\"\n",
        "    # Add in dataframe master_df max_len, accuracy, precision, recall, f1-score\n",
        "    ret = pd.DataFrame(columns=['max_len', 'accuracy', 'precision', 'recall', 'f1-score', 'corpus_size'])\n",
        "    ret.loc[0] = [self.max_joke_size, accuracy, precision, recall, f1, len(self.list_jokes_tokenized)]\n",
        "    ret.to_csv(config['data_path'])\n",
        "    \n",
        "    self.model.save(config['model_path']) # save the model\n",
        "\n",
        "    # save the tokenizer\n",
        "    # with open(config['tokenizer_path'], 'wb') as handle:\n",
        "    #     pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnZZQ_KwBJnt"
      },
      "source": [
        "# Loading data from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZtY_qNJ6RyE",
        "outputId": "f8552b6e-691f-4c96-eec9-fa60ef3d401f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfuly read!\n"
          ]
        }
      ],
      "source": [
        "# path = 'drive/MyDrive/Humour_Detection_Dataset/final_jokes(2918).csv'\n",
        "\n",
        "path = \"dataset/1a_final_jokes(2918).csv\" # DATASET 1a\n",
        "# path = \"dataset/1b_final_jokes(2826)(stopwords)(uppercase).csv\" # DATASET 1b\n",
        "# path = \"dataset/2a_final_jokes_and_facts(2918).csv\" # DATASET 2a\n",
        "# path = \"dataset/2b_final_jokes_and_facts(2826)(stopwords)(uppercase).csv\" # DATASET 2b\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.read_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocfAInVEBRlL"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VNvLgfRBWuw"
      },
      "source": [
        "The functions below enable the data from the csv to be transformed in variables used to compute the tf and idf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLfEWBLjsQNk",
        "outputId": "a4404d17-2f4a-427d-ba21-456df8557d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenisation is complete!\n",
            "The vocabulary has been created!\n"
          ]
        }
      ],
      "source": [
        "joke_model.tokenise_clean_text()\n",
        "vocabulary = joke_model.return_vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT5NoINhBY9F"
      },
      "source": [
        "# Computing Term Frequency (TF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_zRdZXbBc7a"
      },
      "source": [
        "Create TF for each word in the vocabulary:\n",
        "\n",
        "The number of times a word appears in a document (joke) divded by the total number of words in the document (joke). Every document (joke) has its own term frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1OAGRfe5_bi",
        "outputId": "b64061c1-ff99-48ad-ab2d-902a13811a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The vocabulary with the associated frequency for each word has been generated!\n",
            "Term frequency matrix/dictionary was generated!\n"
          ]
        }
      ],
      "source": [
        "# TF\n",
        "# the frequency of each word from the vocabulary in the corpus \n",
        "vocab_freq = joke_model.return_terms_frequencies(vocabulary)\n",
        "# Calculate tf number of times a word appears in a doc / total number of words in the doc\n",
        "tf = joke_model.generate_tf(vocab_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiuU6M0lBgSe"
      },
      "source": [
        "# Computing the Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nlhT-upBker"
      },
      "source": [
        "Calculating IDF for each word:\n",
        "\n",
        "The log of the number of documents divided by the number of documents (jokes) that contain the word w. The IDF of a word is computed once for all documents (jokes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7SQuK9x6ALe",
        "outputId": "23bb8708-1c7a-448b-cc7d-9c8e8ab46dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A list of all documents in which the words from our vocab appear in has been generated!\n",
            "A dictionary with each word's tf-idf values has been generated!\n"
          ]
        }
      ],
      "source": [
        "# IDF\n",
        "# create a list of all the docs in which the vocab words occur -> to be used in idf\n",
        "dict_document_freq = joke_model.document_frequency(vocab_freq)\n",
        "idf = joke_model.generate_idf(dict_document_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4msr8SA6Ipp9"
      },
      "outputs": [],
      "source": [
        "############ For the LIVE DEMO ###############\n",
        "with open(config['tokenizer_path'], 'wb') as handle:\n",
        "    pickle.dump(dict_document_freq, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Jl8emvBnaC"
      },
      "source": [
        "# Computing the TF-IDF vectors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPuuS0BWBrX_"
      },
      "source": [
        "TF-IDF is simply the TF multiplied by IDF.\n",
        "\n",
        "The output will be a dictionary of tf-idf values ({\"word\" : \"joke\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Y7gynL6D5q",
        "outputId": "6c547565-dba9-46ea-cdc2-c9210ae4e96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tf-idf values have beed generated!\n"
          ]
        }
      ],
      "source": [
        "# TF IDF\n",
        "dict_tf_idf = joke_model.generate_tf_idf(vocab_freq, tf, idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt97za8KBuxe"
      },
      "source": [
        "Representing each joke by an embedding vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8eOpD9a6F57",
        "outputId": "287244e5-fc30-4234-e1f1-e56485634d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The jokes have been embedded by their tf-idf values!\n",
            "This is the max joke length for our corpus:  134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\paulc\\AppData\\Local\\Temp/ipykernel_10756/2728720486.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  jokes_as_tf_idf = np.asarray(jokes_as_tf_idf)\n"
          ]
        }
      ],
      "source": [
        "# embed\n",
        "jokes_as_tf_idf_np = joke_model.joke_as_tf_idf(dict_tf_idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q3ygMkIB1fU"
      },
      "source": [
        "# Creating training and testing datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHO4lYwB5D4"
      },
      "source": [
        "Pad the jokes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTlKkgVR6Hu3",
        "outputId": "b16402ed-27f9-4e83-bebf-23b9b0432349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The jokes (input) have been padded to the same size,  134\n",
            "This is the shape of the input:\n",
            " (2918, 134)\n"
          ]
        }
      ],
      "source": [
        "# pad\n",
        "jokes_as_tf_idf_np = joke_model.pad_jokes(jokes_as_tf_idf_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EER-wBQFB8mo"
      },
      "source": [
        "Split the data in training - 70%, validation - 10% and testing - 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uTreR-Ku6J2o"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = joke_model.generate_train_test_data(jokes_as_tf_idf_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5fwF4JuCB5P"
      },
      "source": [
        "# Training and saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjtOaveM6Lh8",
        "outputId": "a45d0972-87b3-4a0f-c96f-3e2d9b49614f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: 2042\n",
            "X_test: 876\n",
            "y_train: 2042\n",
            "y_test: 876\n",
            "Epoch 1/3\n",
            "27/27 [==============================] - 10s 209ms/step - loss: 0.6938 - accuracy: 0.5182 - val_loss: 0.6929 - val_accuracy: 0.4878\n",
            "Epoch 2/3\n",
            "27/27 [==============================] - 5s 203ms/step - loss: 0.6919 - accuracy: 0.5182 - val_loss: 0.6922 - val_accuracy: 0.5415\n",
            "Epoch 3/3\n",
            "27/27 [==============================] - 6s 211ms/step - loss: 0.6899 - accuracy: 0.5465 - val_loss: 0.6916 - val_accuracy: 0.5366\n",
            "The model completed the training step!\n",
            "28/28 [==============================] - 1s 41ms/step - loss: 0.6865 - accuracy: 0.5708\n",
            "Accuracy: 57.08%\n",
            "Precision: 56.10%\n",
            "Recall: 69.53%\n",
            "F1-Score: 62.10%\n"
          ]
        }
      ],
      "source": [
        "joke_model.LSTM_model(X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKd07O13G423"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "2b_tf_idf.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a3a52a35f691380e40dee258b5f482ebea5e8d938b654dd4fe14bb13e1a5bfa7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
