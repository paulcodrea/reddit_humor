{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b_tf_idf.ipynb",
      "provenance": [],
      "mount_file_id": "1n1NVwTzbJcVM7E6J96l2_06n41KRQISl",
      "authorship_tag": "ABX9TyPTHrLxRXYp9nviKGoa88lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/2b_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a tf-idf vector to represent every joke in the corpus"
      ],
      "metadata": {
        "id": "_-KO2OLEQLwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from the csv:"
      ],
      "metadata": {
        "id": "OdXW-7UDQYL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSYc96Ad4shW",
        "outputId": "8e013dc1-e7a4-44e6-e293-cce3c0db6b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Data successfuly read!\n",
            "This is the first line from the data file: \n",
            " ['0', \"My 5 year old just ran out of her room to tell me this joke she just thought up: what did the cow say after he was fed? Moooooooooore!\\n\\nI've never been this proud of anything in my life.\", '1']\n",
            "This is the formatted joke: \n",
            " My 5 year old just ran out of her room to tell me this joke she just thought up: what did the cow say after he was fed? Moooooooooore!\n",
            "\n",
            "I've never been this proud of anything in my life.\n"
          ]
        }
      ],
      "source": [
        "# tf-idf\n",
        "\n",
        "import csv\n",
        "import nltk\n",
        "from nltk import word_tokenize, RegexpTokenizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "with open('drive/MyDrive/Humour_Detection_Dataset/final_jokes(1283).csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "print(\"Data successfuly read!\")\n",
        "\n",
        "print(\"This is the first line from the data file: \\n\", data[1])\n",
        "print(\"This is the formatted joke: \\n\", data[1][1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "6iP56LPWQebX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def removeStopWords( words):\n",
        "        \"\"\"\n",
        "        Remove the stop-words from a list.\n",
        "        \"\"\"\n",
        "        filtered_words = []\n",
        "        for w in words:\n",
        "            if w not in stopWords:\n",
        "                filtered_words.append(w)\n",
        "        return filtered_words\n",
        "\n",
        "def process_document(document: list, lower = True, stem = False, remove_stop_words=True) -> list:\n",
        "        \"\"\"\n",
        "        Pre-process (lower, remove non-alphabetic words, and stem) a document \n",
        "        and return a list of its terms.\n",
        "        \"\"\"\n",
        "        list_jokes_tokenised = []\n",
        "        list_tokens = []\n",
        "        for line in document:\n",
        "          joke = line[1]\n",
        "          tokenizer = RegexpTokenizer(r'\\w+')\n",
        "          tokens = tokenizer.tokenize(joke)\n",
        "          if(lower):\n",
        "              tokens = [w.lower() for w in tokens]\n",
        "          if(stem):\n",
        "              tokens = [stemmer.stem(w) for w in tokens]\n",
        "          if(remove_stop_words):\n",
        "              tokens = removeStopWords(tokens)\n",
        "          list_jokes_tokenised.append(tokens)\n",
        "          list_tokens = list_tokens + tokens\n",
        "        return list_tokens, list_jokes_tokenised\n",
        "\n",
        "def term_frequency_corpus(tokens: list):\n",
        "  \"\"\"\n",
        "  Returns the a dictionary where the keys are the vocabulary words and the values\n",
        "  are the frequency of the word in the whole corpus.\n",
        "  \"\"\"\n",
        "  vocab_freq = {}\n",
        "  vocabulary = list(dict.fromkeys(tokens))\n",
        "  for word in vocabulary:\n",
        "    vocab_freq[word] = 0\n",
        "  for token in tokens:\n",
        "    vocab_freq[token] = vocab_freq[token] + 1\n",
        "  return vocab_freq\n",
        "\n",
        "def term_frequency(vocab: dict, jokes: list):\n",
        "  \"\"\"\n",
        "  Returns a dictionary where the keys are the vocabulary words and the values are\n",
        "  a list that represents the frequency of the word in each joke divided by the \n",
        "  length of the joke.\n",
        "  \"\"\"\n",
        "  dict_term_freq = {}\n",
        "  for word in vocab.keys():\n",
        "    dict_term_freq[word] = []\n",
        "  for word in vocab.keys():\n",
        "    for joke in jokes:\n",
        "      freq = 0\n",
        "      for word_index in range(len(joke)):\n",
        "        if word == joke[word_index]:\n",
        "          freq = freq + 1\n",
        "      dict_term_freq[word].append(freq / len(joke))\n",
        "  return dict_term_freq\n",
        "\n",
        "\n",
        "\n",
        "def document_frequency(vocabulary: dict, jokes: list):\n",
        "  \"\"\"\n",
        "  Returns a dictionary where the keys are all words in the vocabulary and the \n",
        "  values the list of jokes the word appears in.\n",
        "  \"\"\"\n",
        "  doc_freq = {}\n",
        "  for word in vocabulary.keys():\n",
        "    doc_freq[word] = []\n",
        "    for joke_index in range(len(jokes)):\n",
        "      if word in jokes[joke_index]:\n",
        "        doc_freq[word].append(joke_index)\n",
        "  return doc_freq\n",
        "\n"
      ],
      "metadata": {
        "id": "7QM9NSON74V3"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "list_tokens, list_jokes_tokenized = process_document(data, lower = True, stem = False, remove_stop_words=False)\n",
        "print(\"The data has been changed into tokens!\")\n",
        "# get rid of the first row (titles)\n",
        "list_tokens = list_tokens[1:]\n",
        "list_jokes_tokenized = list_jokes_tokenized[1:]\n",
        "\n",
        "print(\"This is the first token from the corpus, pre-processed: \\n\", list_tokens[0])\n",
        "print(\"This is the first item from the tokenised list of jokes:\\n\", list_jokes_tokenized[0])\n",
        "print(\"This is how many jokes the data set has:\\n\",len(list_jokes_tokenized))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXR2UQW8-y8",
        "outputId": "51b4e84e-141e-463e-f45e-d6241b9e67aa"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data has been changed into tokens!\n",
            "This is the first token from the corpus, pre-processed: \n",
            " my\n",
            "This is the first item from the tokenised list of jokes:\n",
            " ['my', '5', 'year', 'old', 'just', 'ran', 'out', 'of', 'her', 'room', 'to', 'tell', 'me', 'this', 'joke', 'she', 'just', 'thought', 'up', 'what', 'did', 'the', 'cow', 'say', 'after', 'he', 'was', 'fed', 'moooooooooore', 'i', 've', 'never', 'been', 'this', 'proud', 'of', 'anything', 'in', 'my', 'life']\n",
            "This is how many jokes the data set has:\n",
            " 1283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "at this point we have:\n",
        "\n",
        "list_tokens -> to be used to generate the vocab\n",
        "\n",
        "list_jokes_tokenized -> to be used to generate embeddings!"
      ],
      "metadata": {
        "id": "tLpplMybWPI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF calculation"
      ],
      "metadata": {
        "id": "4K1aN4gdRTYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TF for each word in the vocabulary:\n",
        "\n",
        "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
      ],
      "metadata": {
        "id": "rnrgkDZYRKeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take 2\n",
        "# create TF (term frequency)\n",
        "\n",
        "# the frequency of each word from the vocabulary in the corpus \n",
        "vocab_freq = term_frequency_corpus(list_tokens)\n",
        "print(\"The vocabulary with the associated frequency for each word has been generated!\")\n",
        "print(\"This is how many times the word 'joke' is in our corpus\\n\", vocab_freq[\"joke\"])\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJigOi636i2r",
        "outputId": "e9c8240f-c7ce-43c2-c1b4-e70a88abf594"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary with the associated frequency for each word has been generated!\n",
            "This is how many times the word 'joke' is in our corpus\n",
            " 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(vocab_freq)\n",
        "print(\"This is the length of our vocabulary:\\n\", vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9V-ZLr_2QSY",
        "outputId": "6f6a1e62-9fb2-4159-b636-fb05fefbcc56"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the length of our vocabulary:\n",
            " 4708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate tf number of times a word appears in a doc / total number of words in the doc\n",
        "tf = term_frequency(vocab_freq, list_jokes_tokenized)\n",
        "print(\"Term frequency matrix/dictionary was generated!\")\n",
        "print(\"This is the term frequency vector for word 'joke':\\n\", tf[\"joke\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB4hHCsXIpTM",
        "outputId": "c4070d68-30ca-4ffa-dbbf-f3c31331eaed"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term frequency matrix/dictionary was generated!\n",
            "This is the term frequency vector for word 'joke':\n",
            " [0.025, 0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019230769230769232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038461538461538464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038461538461538464, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024390243902439025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05555555555555555, 0.0, 0.0, 0.0, 0.029411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047619047619047616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06451612903225806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255813953488372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009900990099009901, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047619047619047616, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05555555555555555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.058823529411764705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IDF calculation"
      ],
      "metadata": {
        "id": "RwFZQ0KcRcx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating IDF for each word:\n",
        "\n",
        "The log of the number of documents divided by the number of documents that contain the word w. The IDF is computed once for all documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "wUHwDnEYRewS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "# create a list of all the docs in which the vocab words occur -> to be used in idf\n",
        "dict_document_freq = document_frequency(vocab_freq, list_jokes_tokenized)\n",
        "print(\"A list of all documents in which the words from our vocab appear in have been generated!\")\n",
        "print(\"The list of all documents that contain the word 'Joke' looks like this:\\n\",dict_document_freq[\"joke\"] )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLOBMkJHI1VD",
        "outputId": "6f4e6442-a59f-4eab-db6e-af79754b7678"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A list of all documents in which the words from our vocab appear in have been generated!\n",
            "The list of all documents that contain the word 'Joke' looks like this:\n",
            " [0, 3, 36, 54, 92, 133, 135, 163, 213, 222, 228, 243, 256, 291, 355, 467, 493, 557, 568, 590, 597, 599, 617, 664, 728, 732, 753, 818, 822, 832, 833, 880, 909, 958, 963, 975, 1003, 1036, 1044, 1051, 1060, 1084, 1089, 1096, 1106, 1160, 1179, 1186, 1200, 1213, 1241, 1271, 1273]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "idf = {}\n",
        "for word in dict_document_freq.keys():\n",
        "  idf[word] = math.log(len(list_jokes_tokenized)/len(dict_document_freq[word]))\n",
        "print(\"A dictionary with each word's tf-idf values has been generated!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Xv4YplZbQD",
        "outputId": "cc5d79d1-d70f-4bf0-f611-35a444d4bdf8"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A dictionary with each word's tf-idf values has been generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF calculation"
      ],
      "metadata": {
        "id": "Wwgfz6_ZRn8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is simply the TF multiplied by IDF.\n",
        "\n",
        "The output will be a list of tf-idf values (word x joke)"
      ],
      "metadata": {
        "id": "nha9w2c4RqHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_tf_idf = {}\n",
        "\n",
        "for word in vocab_freq.keys():\n",
        "  dict_tf_idf[word] = []\n",
        "  for tf_index in range(len(tf[word])):\n",
        "    dict_tf_idf[word].append(tf[word][tf_index] * idf[word])\n",
        "\n",
        "print(\"The tf-idf values have beed generated! \")\n",
        "print(\"this is how the tf-idf vector looks like for the word 'joke':\\n\",dict_tf_idf[\"joke\"])\n",
        "print(\"This is the length of each tf-idf vector. Each value represent's the word's tf-idf for each sentence:\\n\", len(dict_tf_idf[\"joke\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apciYnoNRaCw",
        "outputId": "f118a8fe-6c4e-456f-b2f4-135269b747b9"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tf-idf values have beed generated! \n",
            "this is how the tf-idf vector looks like for the word 'joke':\n",
            " [0.07966661127658786, 0.0, 0.0, 0.07966661127658786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24512803469719344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06128200867429836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12256401734859672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2276188893616796, 0.0, 0.2655553709219595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.531110741843919, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24512803469719344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3354383632698436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896967682785013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2655553709219595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19916652819146966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15933322255317572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21244429673756762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10622214836878381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15933322255317572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24512803469719344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2276188893616796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4552377787233592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12256401734859672, 0.0, 0.2276188893616796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09958326409573483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07772352319667108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17703691394797302, 0.0, 0.0, 0.0, 0.09372542503127984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896967682785013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21244429673756762, 0.0, 0.0, 0.0, 0.19916652819146966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6373328902127029, 1.5933322255317572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09372542503127984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.062221483687838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24512803469719344, 0.0, 0.0, 0.0, 0.0, 0.531110741843919, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.062221483687838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15174592624111974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6373328902127029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20559125490732352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410847560612824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031551133178846676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15174592624111974, 0.0, 0.0, 0.0, 0.0, 0.2276188893616796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2896967682785013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5933322255317572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4552377787233592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.531110741843919, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19916652819146966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17703691394797302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12746657804254058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24512803469719344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4552377787233592, 0.0, 0.18745085006255968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "This is the length of each tf-idf vector. Each value represent's the word's tf-idf for each sentence:\n",
            " 1283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representing each joke with tf-idf values"
      ],
      "metadata": {
        "id": "0RzQ3Hn9Ufyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing each joke by and embedding vector "
      ],
      "metadata": {
        "id": "0myX5LnTbBn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_as_tf_idf = []\n",
        "max_joke_size = 0\n",
        "print(type(list_jokes_tokenized))\n",
        "for joke_index in range(len(list_jokes_tokenized)):\n",
        "  joke = list_jokes_tokenized[joke_index]\n",
        "  tf_idf_list = []\n",
        "  for word in joke:\n",
        "    tf_idf_list.append(dict_tf_idf[word][joke_index])\n",
        "  if len(joke) > max_joke_size:\n",
        "    max_joke_size = len(tf_idf_list)\n",
        "  tf_idf = np.asarray(tf_idf_list)\n",
        "  jokes_as_tf_idf.append(tf_idf)\n",
        "jokes_as_tf_idf = np.asarray(jokes_as_tf_idf)\n",
        "print(type(jokes_as_tf_idf))\n",
        "print(type(jokes_as_tf_idf[0]))\n",
        "print(\"The jokes have been embedded by their tf-idf values!\")\n",
        "print(\"This is the max joke length for our corpus: \", max_joke_size)\n",
        "print(\"This is how the first joke looks like before the embedding step: \\n\", list_jokes_tokenized[0])\n",
        "print(\"This is how the tf-idf embedding of the first joke looks like:\\n\",jokes_as_tf_idf[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_CQ7CirUe2o",
        "outputId": "796a429e-7c34-4dc6-b6e7-f4b33dbc2b2d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "The jokes have been embedded by their tf-idf values!\n",
            "This is the max joke length for our corpus:  347\n",
            "This is how the first joke looks like before the embedding step: \n",
            " ['my', '5', 'year', 'old', 'just', 'ran', 'out', 'of', 'her', 'room', 'to', 'tell', 'me', 'this', 'joke', 'she', 'just', 'thought', 'up', 'what', 'did', 'the', 'cow', 'say', 'after', 'he', 'was', 'fed', 'moooooooooore', 'i', 've', 'never', 'been', 'this', 'proud', 'of', 'anything', 'in', 'my', 'life']\n",
            "This is how the tf-idf embedding of the first joke looks like:\n",
            " [0.08402464 0.12399329 0.11122265 0.10809358 0.15230413 0.14426655\n",
            " 0.07784763 0.08816644 0.09228051 0.13412992 0.03487262 0.10666462\n",
            " 0.06096144 0.13068308 0.07966661 0.08014282 0.15230413 0.11122265\n",
            " 0.07615206 0.03177131 0.05685386 0.02131269 0.10960919 0.07200726\n",
            " 0.09947256 0.05158015 0.04801286 0.16159523 0.17892391 0.02864223\n",
            " 0.0974715  0.09307423 0.10531293 0.13068308 0.14426655 0.08816644\n",
            " 0.12693787 0.04597091 0.08402464 0.13868796]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_as_tf_idf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyU0rwhrdOSS",
        "outputId": "508f62e4-94a9-4e3c-bc32-1919505101bb"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1283,)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split train - test data"
      ],
      "metadata": {
        "id": "qjk2HGN6guhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "select random indices for train"
      ],
      "metadata": {
        "id": "BPyMpXQJgxjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "indices = list(range(1, len(jokes_as_tf_idf)))\n",
        "\n",
        "number_of_training_samples = int(len(jokes_as_tf_idf) * 0.7)\n",
        "training_index_list = random.sample(indices,number_of_training_samples)\n",
        "\n",
        "testing_index_list = list(set(indices) ^ set(training_index_list))  # we r missing one joke (bc of the int())\n",
        "random.shuffle(training_index_list)\n",
        "random.shuffle(testing_index_list)"
      ],
      "metadata": {
        "id": "5094DNBFgwoq"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jokes_as_tf_idf[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUj22wYQfl83",
        "outputId": "d3057e6b-3c30-49cb-d111-7f5df31e59e1"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08402464 0.12399329 0.11122265 0.10809358 0.15230413 0.14426655\n",
            " 0.07784763 0.08816644 0.09228051 0.13412992 0.03487262 0.10666462\n",
            " 0.06096144 0.13068308 0.07966661 0.08014282 0.15230413 0.11122265\n",
            " 0.07615206 0.03177131 0.05685386 0.02131269 0.10960919 0.07200726\n",
            " 0.09947256 0.05158015 0.04801286 0.16159523 0.17892391 0.02864223\n",
            " 0.0974715  0.09307423 0.10531293 0.13068308 0.14426655 0.08816644\n",
            " 0.12693787 0.04597091 0.08402464 0.13868796]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad the jokes"
      ],
      "metadata": {
        "id": "I9hbs2j4zTMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_arr = np.zeros((len(jokes_as_tf_idf), max_joke_size))\n",
        "\n",
        "for idx, joke in enumerate(jokes_as_tf_idf):\n",
        "  len_joke = len(joke)\n",
        "  joke_x = jokes_as_tf_idf[idx]\n",
        "  new_arr[idx] = np.append(joke, [0] * (max_joke_size - len_joke))\n",
        "\n",
        "print(new_arr.shape)\n",
        "jokes_as_tf_idf = new_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0z3NCHzvEVY",
        "outputId": "521e7ed2-fba9-4b1e-bdcc-a9aac98d484c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1283, 347)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(jokes_as_tf_idf[0])"
      ],
      "metadata": {
        "id": "H6ctLGodw1HV"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_train_test_data(jokes_as_tf_idf, index_list, corpus):\n",
        "  returned_data = np.zeros(shape=(len(index_list), max_joke_size))\n",
        "  labels = np.zeros(shape=(len(index_list), 1))\n",
        "  index_returned_data = 0\n",
        "  for index in index_list:\n",
        "    returned_data[index_returned_data] = jokes_as_tf_idf[index]\n",
        "    labels[index_returned_data] = corpus[index][2]\n",
        "    index_returned_data += 1\n",
        "\n",
        "  return returned_data, labels\n",
        " "
      ],
      "metadata": {
        "id": "nvXaf66eiwnR"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, y_train = generate_train_test_data(jokes_as_tf_idf, training_index_list, data)\n",
        "X_test, y_test = generate_train_test_data(jokes_as_tf_idf, testing_index_list, data)\n",
        "\n",
        "# print(X_train[0])\n",
        "# print(y_train[0])\n",
        "# print(X_test[0])\n",
        "# print(y_test[0])"
      ],
      "metadata": {
        "id": "qykm6HeWoGmm"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying it on the LSTM"
      ],
      "metadata": {
        "id": "-Wjbdq4sir4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHfVRE5IiuRp",
        "outputId": "3b9ff87f-cf30-4bc0-d674-09eb870e2af0"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 3, \n",
        "    \"batch_size\": 70,\n",
        "    \"test_p\": 0.2,\n",
        "    \"val_p\": 0.1,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'softmax',\n",
        "}"
      ],
      "metadata": {
        "id": "vzIGRYMWi15r"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_model(max_length, X_train, X_test, y_train, y_test, vocabulary_size):\n",
        "  \"\"\"\n",
        "  Splits the data into train and validation sets.\n",
        "\n",
        "  Constructs the LSTM model.\n",
        "  \"\"\"\n",
        "\n",
        "  # print(\"X shape: \", X.shape)\n",
        "  # print(\"y shape: \", y.shape)\n",
        "\n",
        "  # split the data into train and validation sets and make them random\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config['test_p'], random_state=42)\n",
        "\n",
        "  print(\"X_train:\", len(X_train))\n",
        "  print(\"X_test:\", len(X_test))\n",
        "  print(\"y_train:\", len(y_train))\n",
        "  print(\"y_test:\", len(y_test))\n",
        "\n",
        "  print(X_train.shape)\n",
        "  print(y_train.shape)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocabulary_size, output_dim=max_length, input_length=int(X_train.shape[1])))\n",
        "  model.add(Dropout(config['Dropout_layer'][0]))\n",
        "  model.add(LSTM(config['LSTM_layer'][1], activation=config['activation']))\n",
        "  model.add(Dropout(config['Dropout_layer'][1]))\n",
        "  model.add(Dense(units=1, activation=config['activation']))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto', validation_split=config['val_p'])\n",
        "\n",
        "  # Evaluate the model\n",
        "  scores = model.evaluate(X_test, y_test)\n",
        "  print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "  # Print Precision and Recall\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred = np.round(y_pred)\n",
        "  print(\"Precision: %.2f%%\" % (precision_score(y_test, y_pred) * 100))\n",
        "  print(\"Recall: %.2f%%\" % (recall_score(y_test, y_pred) * 100))\n",
        "  print(\"F1-Score: %.2f%%\" % (f1_score(y_test, y_pred) * 100))"
      ],
      "metadata": {
        "id": "A583qWMsi36P"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model(max_joke_size, X_train, X_test, y_train, y_test, len(vocab_freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-yY7GlWi6dW",
        "outputId": "f03ae2b6-e7d3-4ea0-b1d0-c25d77b9c2b5"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: 898\n",
            "X_test: 384\n",
            "y_train: 898\n",
            "y_test: 384\n",
            "(898, 347)\n",
            "(898, 1)\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 1s/step - loss: 0.6931 - accuracy: 0.5322 - val_loss: 0.6929 - val_accuracy: 0.5556\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 14s 1s/step - loss: 0.6929 - accuracy: 0.5322 - val_loss: 0.6923 - val_accuracy: 0.5556\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 14s 1s/step - loss: 0.6927 - accuracy: 0.5322 - val_loss: 0.6920 - val_accuracy: 0.5556\n",
            "12/12 [==============================] - 2s 176ms/step - loss: 0.6926 - accuracy: 0.5286\n",
            "Accuracy: 52.86%\n",
            "Precision: 52.86%\n",
            "Recall: 100.00%\n",
            "F1-Score: 69.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SivgGfRGqYkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}