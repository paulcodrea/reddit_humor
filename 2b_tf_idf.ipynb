{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b_tf_idf.ipynb",
      "provenance": [],
      "mount_file_id": "1n1NVwTzbJcVM7E6J96l2_06n41KRQISl",
      "authorship_tag": "ABX9TyPSGH3dIUL2doZhOx8lOePk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/2b_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM with a tf-idf embedding vector "
      ],
      "metadata": {
        "id": "_-KO2OLEQLwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports + Config"
      ],
      "metadata": {
        "id": "gVZBXpowG1Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk import word_tokenize, RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding, TextVectorization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYGl01tRxB7",
        "outputId": "eaf798e7-1b72-4139-a277-d468de2d103b"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 3, \n",
        "    \"batch_size\": 70,\n",
        "    \"test_p\": 0.2,\n",
        "    \"val_p\": 0.1,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'sigmoid',\n",
        "    ##################### SAVE FOR LIVE DEMO #############################\n",
        "    \"model_path\": './model/2b_model.h5',\n",
        "    \"tokenizer_path\": './model/2b_tokenizer.pickle',\n",
        "    \"data_path\": \"model/2b_data.csv\",\n",
        "}"
      ],
      "metadata": {
        "id": "vzIGRYMWi15r"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods:"
      ],
      "metadata": {
        "id": "OdXW-7UDQYL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_model:\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "    self.data = pd.DataFrame()\n",
        "    self.max_joke_size = 0\n",
        "    self.vocabulary_size = 0\n",
        "    self.list_jokes_tokenized = []\n",
        "    self.model = None\n",
        "\n",
        "  def read_dataset(self):\n",
        "    self.data = pd.read_csv(self.path)\n",
        "    print(\"Data successfuly read!\")\n",
        "    # print(\"This is the first line from the data file: \\n\", self.data.head(1))\n",
        "\n",
        "  def tokenise_clean_text(self):\n",
        "    \"\"\"\n",
        "    Tokenises the data from the CSV file.\n",
        "    Since the CSV has a \"clean_text\" column, the only required pre-processing\n",
        "    is splitting it from white spaces.\n",
        "    \"\"\"\n",
        "    arr = []\n",
        "    for line in self.data['clean_text']:\n",
        "      arr.append(line.split(' '))\n",
        "    self.list_jokes_tokenized = [x[:(len(x) - 1)] for x in arr] # remove '' at the end\n",
        "    print('Tokenisation is complete!')\n",
        "    # print(\"This is the first item from the tokenised list of jokes:\\n\", self.list_jokes_tokenized[0])\n",
        "    # print(\"This is how many jokes the data set has:\\n\",len(self.list_jokes_tokenized))\n",
        "\n",
        "  def return_vocab(self) -> list:\n",
        "    \"\"\"\n",
        "    Retuns the vocabulary of the document.\n",
        "    \"\"\"\n",
        "    computed_vocab = []\n",
        "    for line in self.list_jokes_tokenized:\n",
        "      for word in line:\n",
        "        computed_vocab.append(word)\n",
        "    computed_vocab = list(dict.fromkeys(computed_vocab))\n",
        "\n",
        "    print(\"The vocabulary has been created!\")\n",
        "    # print(\"This is the len of the vocabulary:\\n \", len(computed_vocab))\n",
        "    # print(\"This is the first token from the corpus, pre-processed: \\n\", computed_vocab[0])\n",
        "\n",
        "    self.vocabulary_size = len(computed_vocab)\n",
        "    return computed_vocab\n",
        "\n",
        "  def return_terms_frequencies(self, vocabulary):\n",
        "    \"\"\"\n",
        "    Returns the term frequency dictionary. The keys are the vocabulary words and\n",
        "    the values are the frequency of the words in the whole corpus.\n",
        "    \"\"\"\n",
        "    vocab_freq = {}\n",
        "    for word in vocabulary:\n",
        "      vocab_freq[word] = 0\n",
        "    for joke in self.list_jokes_tokenized:\n",
        "      for token in joke:\n",
        "        vocab_freq[token] = vocab_freq[token] + 1\n",
        "    \n",
        "    print(\"The vocabulary with the associated frequency for each word has been generated!\")\n",
        "    # print(\"This is how many times the word 'joke' is in our corpus\\n\", vocab_freq[\"joke\"])\n",
        "    \n",
        "    return vocab_freq\n",
        "\n",
        "  def generate_tf(self, vocab: dict):\n",
        "    \"\"\"\n",
        "    Returns a term frequency dictionary. The keys are the vocabulary words and \n",
        "    the values are a list that represents the frequency of the word in each joke\n",
        "    divided by the length of the joke.\n",
        "    \"\"\"\n",
        "    dict_term_freq = {}\n",
        "    for word in vocab.keys():\n",
        "      dict_term_freq[word] = []\n",
        "    for word in vocab.keys():\n",
        "      for joke in self.list_jokes_tokenized:\n",
        "        word_freq = 0\n",
        "        for word_index in range(len(joke)):\n",
        "          if word == joke[word_index]:\n",
        "            word_freq = word_freq + 1\n",
        "        dict_term_freq[word].append(word_freq / len(joke))\n",
        "\n",
        "    print(\"Term frequency matrix/dictionary was generated!\")\n",
        "    # print(\"This is the term frequency vector for word 'joke':\\n\", dict_term_freq[\"joke\"])\n",
        "\n",
        "    return dict_term_freq\n",
        "\n",
        "  def document_frequency(self, vocabulary: dict):\n",
        "    \"\"\"\n",
        "    Returns a document frquency dictionary. The keys are all words in the \n",
        "    vocabulary and the values are a list of jokes the word appears in.\n",
        "    \"\"\"\n",
        "    doc_freq = {}\n",
        "    for word in vocabulary.keys():\n",
        "      doc_freq[word] = []\n",
        "      for joke_index in range(len(self.list_jokes_tokenized)):\n",
        "        if word in self.list_jokes_tokenized[joke_index]:\n",
        "          doc_freq[word].append(joke_index)\n",
        "\n",
        "    print(\"A list of all documents in which the words from our vocab appear in has been generated!\")\n",
        "    # print(\"The list of all documents that contain the word 'Joke' looks like this:\\n\",doc_freq[\"joke\"] )\n",
        "\n",
        "    return doc_freq\n",
        "\n",
        "  def generate_idf(self, document_frequencies):\n",
        "    \"\"\"\n",
        "    Returns an inverse document frequency dictionary. The keys are the vocabulary\n",
        "    words and the values are the corresponding idfs.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    for word in document_frequencies.keys():\n",
        "      idf[word] = math.log(len(self.list_jokes_tokenized)/len(document_frequencies[word]))\n",
        "\n",
        "    print(\"A dictionary with each word's tf-idf values has been generated!\")\n",
        "\n",
        "    return idf\n",
        "\n",
        "  def generate_tf_idf(self, vocab_frequencies, tf, idf):\n",
        "    \"\"\"\n",
        "    Returns a tf-idf dictionary. The keys are the vocabulary words and the \n",
        "    values are the corresponding tf-idfs.\n",
        "    \"\"\"\n",
        "    dict_tf_idf = {}\n",
        "\n",
        "    for word in vocab_frequencies.keys():\n",
        "      dict_tf_idf[word] = []\n",
        "      for tf_index in range(len(tf[word])):\n",
        "        dict_tf_idf[word].append(tf[word][tf_index] * idf[word])\n",
        "\n",
        "    print(\"The tf-idf values have beed generated!\")\n",
        "    # print(\"this is how the tf-idf vector looks like for the word 'joke':\\n\",dict_tf_idf[\"joke\"])\n",
        "    # print(\"This is the length of each tf-idf vector. Each value represent's the word's tf-idf for each sentence:\\n\", len(dict_tf_idf[\"joke\"]))\n",
        "    \n",
        "    return dict_tf_idf\n",
        "\n",
        "  def joke_as_tf_idf(self, dict_tf_idf):\n",
        "    \"\"\"\n",
        "    Returns a list of jokes represented by the words' tf-idf values.\n",
        "    \"\"\"\n",
        "    jokes_as_tf_idf = []\n",
        "\n",
        "    for joke_index in range(len(self.list_jokes_tokenized)):\n",
        "      joke = self.list_jokes_tokenized[joke_index]\n",
        "      tf_idf_list = []\n",
        "      for word in joke:\n",
        "        tf_idf_list.append(dict_tf_idf[word][joke_index])\n",
        "      tf_idf = np.asarray(tf_idf_list)\n",
        "      jokes_as_tf_idf.append(tf_idf)\n",
        "\n",
        "      # calculate max_joke size to use in the padding step\n",
        "      if len(joke) > self.max_joke_size:\n",
        "        self.max_joke_size = len(tf_idf_list)\n",
        "        \n",
        "    jokes_as_tf_idf = np.asarray(jokes_as_tf_idf)\n",
        "\n",
        "    print(\"The jokes have been embedded by their tf-idf values!\")\n",
        "    print(\"This is the max joke length for our corpus: \", self.max_joke_size)\n",
        "    # print(\"This is how the first joke looks like before the embedding step: \\n\", self.list_jokes_tokenized[0])\n",
        "    # print(\"This is how the tf-idf embedding of the first joke looks like:\\n\",jokes_as_tf_idf[0])\n",
        "\n",
        "    return jokes_as_tf_idf\n",
        "\n",
        "  def pad_jokes(self, jokes_as_tf_idf_np):\n",
        "    \"\"\"\n",
        "    Returns the input padded to the max_length of the corpus. The padding will\n",
        "    be at the beginning of the input, and represented by the value 0.\n",
        "    \"\"\"\n",
        "    new_arr = np.zeros((len(jokes_as_tf_idf_np), self.max_joke_size))\n",
        "    for idx, joke in enumerate(jokes_as_tf_idf_np):\n",
        "      len_joke = len(joke)\n",
        "      joke_x = jokes_as_tf_idf_np[idx]\n",
        "      new_arr[idx] = np.append([0] * (self.max_joke_size - len_joke), joke)\n",
        "\n",
        "    print(\"The jokes (input) have been padded to the same size, \", self.max_joke_size)\n",
        "    print(\"This is the shape of the input:\\n\", new_arr.shape)\n",
        "\n",
        "    return new_arr\n",
        "\n",
        "  def generate_train_test_data(self, jokes_list):\n",
        "    \"\"\"\n",
        "    Returns the training and testing data. The data has been shuffled in the\n",
        "    pre-processing step, so the function simply returns 80/20 training/testing\n",
        "    data.\n",
        "    \"\"\"\n",
        "    train_max_index = int(len(jokes_list) * 0.8)\n",
        "    X_train = np.zeros(shape=(train_max_index, self.max_joke_size))\n",
        "    X_test = np.zeros(shape=(len(jokes_list) - train_max_index, self.max_joke_size))\n",
        "    index_returned_data = 0\n",
        "    \n",
        "    # training data\n",
        "    for index in range(0, train_max_index):\n",
        "      X_train[index_returned_data] = jokes_list[index]\n",
        "      index_returned_data = index_returned_data + 1\n",
        "    y_train = self.data[\"funny\"][:train_max_index]\n",
        "    # testing data\n",
        "    index_returned_data = 0\n",
        "    for index in range(train_max_index, len(jokes_list)):\n",
        "      X_test[index_returned_data] = jokes_list[index]\n",
        "      index_returned_data = index_returned_data + 1\n",
        "    y_test = self.data[\"funny\"][train_max_index:]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "  def LSTM_model(self, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Constructs and evaluates the LSTM model.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"X_train:\", len(X_train))\n",
        "    print(\"X_test:\", len(X_test))\n",
        "    print(\"y_train:\", len(y_train))\n",
        "    print(\"y_test:\", len(y_test))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=self.vocabulary_size, output_dim=self.max_joke_size, input_length=int(X_train.shape[1])))\n",
        "    model.add(LSTM(50, return_sequences=True))\n",
        "    model.add(LSTM(10))\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation=config['activation']))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto', validation_split=config['val_p'])\n",
        "    \n",
        "    print(\"The model completed the training step!\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(X_test, y_test)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "    # Print Precision and Recall\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.round(y_pred)\n",
        "    precision = precision_score(y_test, y_pred) * 100\n",
        "    recall = recall_score(y_test, y_pred) * 100\n",
        "    f1 = f1_score(y_test, y_pred) * 100\n",
        "    print(\"Precision: %.2f%%\" % (precision))\n",
        "    print(\"Recall: %.2f%%\" % (recall))\n",
        "    print(\"F1-Score: %.2f%%\" % (f1))\n",
        "\n",
        "    # self.save_data(scores[1], precision, recall, f1)\n",
        "\n",
        "\n",
        "  def save_data(self, accuracy, precision, recall, f1):\n",
        "    \"\"\"\n",
        "    Saves the data.\n",
        "    \"\"\"\n",
        "    # Add in dataframe master_df max_len, accuracy, precision, recall, f1-score\n",
        "    ret = pd.DataFrame(columns=['max_len', 'accuracy', 'precision', 'recall', 'f1-score'])\n",
        "    ret.loc[0] = [self.max_joke_size, accuracy, precision, recall, f1]\n",
        "    ret.to_csv(config['data_path'])\n",
        "    \n",
        "    # self.model.save(config['model_path']) # save the model\n",
        "\n",
        "    # save the tokenizer\n",
        "    # with open(config['tokenizer_path'], 'wb') as handle:\n",
        "    #     pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "ixXhMeXke24M"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from the dataset:"
      ],
      "metadata": {
        "id": "pnZZQ_KwBJnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'drive/MyDrive/Humour_Detection_Dataset/final_jokes(2918).csv'\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.read_dataset()"
      ],
      "metadata": {
        "id": "7ZtY_qNJ6RyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bf81ee-68f0-45a4-ada7-9c6222bb730e"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfuly read!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "ocfAInVEBRlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions below enable the data from the csv to be transformed in variables used to compute the tf and idf."
      ],
      "metadata": {
        "id": "1VNvLgfRBWuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joke_model.tokenise_clean_text()\n",
        "vocabulary = joke_model.return_vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLfEWBLjsQNk",
        "outputId": "63e8fbe9-9c77-4f0c-fb99-7e92c5a42523"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisation is complete!\n",
            "The vocabulary has been created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Term Frequency (TF)"
      ],
      "metadata": {
        "id": "JT5NoINhBY9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TF for each word in the vocabulary:\n",
        "\n",
        "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
      ],
      "metadata": {
        "id": "c_zRdZXbBc7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF\n",
        "# the frequency of each word from the vocabulary in the corpus \n",
        "vocab_freq = joke_model.return_terms_frequencies(vocabulary)\n",
        "# Calculate tf number of times a word appears in a doc / total number of words in the doc\n",
        "tf = joke_model.generate_tf(vocab_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1OAGRfe5_bi",
        "outputId": "92908369-a7af-439e-8fb7-f018fe52187d"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary with the associated frequency for each word has been generated!\n",
            "Term frequency matrix/dictionary was generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing the Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "MiuU6M0lBgSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating IDF for each word:\n",
        "\n",
        "The log of the number of documents divided by the number of documents that contain the word w. The IDF of a word is computed once for all documents."
      ],
      "metadata": {
        "id": "6nlhT-upBker"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IDF\n",
        "# create a list of all the docs in which the vocab words occur -> to be used in idf\n",
        "dict_document_freq = joke_model.document_frequency(vocab_freq)\n",
        "idf = joke_model.generate_idf(dict_document_freq)"
      ],
      "metadata": {
        "id": "L7SQuK9x6ALe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748265bc-d30a-4656-add7-1fa3ccfc3fcb"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A list of all documents in which the words from our vocab appear in has been generated!\n",
            "A dictionary with each word's tf-idf values has been generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing the TF-IDF vectors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H6Jl8emvBnaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is simply the TF multiplied by IDF.\n",
        "\n",
        "The output will be a list of tf-idf values (word x joke)"
      ],
      "metadata": {
        "id": "fPuuS0BWBrX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF IDF\n",
        "dict_tf_idf = joke_model.generate_tf_idf(vocab_freq, tf, idf)"
      ],
      "metadata": {
        "id": "I3Y7gynL6D5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ec07ab-af24-4cb7-d258-f8cac6987834"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tf-idf values have beed generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing each joke by an embedding vector.\n"
      ],
      "metadata": {
        "id": "Jt97za8KBuxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embed\n",
        "jokes_as_tf_idf_np = joke_model.joke_as_tf_idf(dict_tf_idf)"
      ],
      "metadata": {
        "id": "A8eOpD9a6F57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655595d7-58fb-494d-cce4-e7853c286da8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The jokes have been embedded by their tf-idf values!\n",
            "This is the max joke length for our corpus:  134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:151: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating training and testing datasets"
      ],
      "metadata": {
        "id": "1Q3ygMkIB1fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad the jokes. \n",
        "\n"
      ],
      "metadata": {
        "id": "EhHO4lYwB5D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pad\n",
        "jokes_as_tf_idf_np = joke_model.pad_jokes(jokes_as_tf_idf_np)"
      ],
      "metadata": {
        "id": "WTlKkgVR6Hu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16402ed-27f9-4e83-bebf-23b9b0432349"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The jokes (input) have been padded to the same size,  134\n",
            "This is the shape of the input:\n",
            " (2918, 134)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data in training - 80% and testing - 20%."
      ],
      "metadata": {
        "id": "EER-wBQFB8mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = joke_model.generate_train_test_data(jokes_as_tf_idf_np)"
      ],
      "metadata": {
        "id": "uTreR-Ku6J2o"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and saving the model"
      ],
      "metadata": {
        "id": "y5fwF4JuCB5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joke_model.LSTM_model(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "AjtOaveM6Lh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45d0972-87b3-4a0f-c96f-3e2d9b49614f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: 2334\n",
            "X_test: 584\n",
            "y_train: 2334\n",
            "y_test: 584\n",
            "Epoch 1/3\n",
            "30/30 [==============================] - 8s 166ms/step - loss: 0.6932 - accuracy: 0.5133 - val_loss: 0.6927 - val_accuracy: 0.5256\n",
            "Epoch 2/3\n",
            "30/30 [==============================] - 4s 135ms/step - loss: 0.6903 - accuracy: 0.5262 - val_loss: 0.6888 - val_accuracy: 0.5214\n",
            "Epoch 3/3\n",
            "30/30 [==============================] - 4s 134ms/step - loss: 0.6883 - accuracy: 0.5448 - val_loss: 0.6866 - val_accuracy: 0.5342\n",
            "The model completed the training step!\n",
            "19/19 [==============================] - 0s 23ms/step - loss: 0.6866 - accuracy: 0.6010\n",
            "Accuracy: 60.10%\n",
            "Precision: 60.87%\n",
            "Recall: 68.18%\n",
            "F1-Score: 64.32%\n"
          ]
        }
      ]
    }
  ]
}