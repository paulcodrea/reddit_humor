{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7bPdvM21K26",
        "outputId": "148a83a7-8e27-4e91-d9d5-64d6a3dd1109"
      },
      "outputs": [],
      "source": [
        "# !pip install gensim==4.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BV5vt3eu1K3A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# import Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwcFzKWjaOaq",
        "outputId": "69a7344f-e477-46c5-882b-8fa2ddb221f8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4n2Gsn6p1K3B"
      },
      "outputs": [],
      "source": [
        "# We might have to change the following\n",
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 3, \n",
        "    \"batch_size\": 70,\n",
        "    \"test_p\": 0.2,\n",
        "    \"val_p\": 0.1,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'softmax',\n",
        "    ##################### SAVE FOR LIVE DEMO #############################\n",
        "    \"model_path\": \"model/2c_model.h5\",\n",
        "    \"tokenizer_path\": \"model/2c_tokenizer.pickle\",\n",
        "    \"data_path\": \"model/2c_data.csv\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "suAtbOfz1K3C"
      },
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "    def __init__(self, path):\n",
        "        self.path = path # Path to the dataset\n",
        "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
        "        self.tokenizer = Tokenizer(num_words=None, split=' ') # Tokenizer to tokenize the text\n",
        "\n",
        "        self.context_window = 5 # Context window size\n",
        "        self.w2v_feature_vector = []\n",
        "        self.vocabulary_size = 0\n",
        "        self.vocabulary = []\n",
        "\n",
        "        self.jokes_to_numerical = []\n",
        "        self.model = None\n",
        "        self.word_vec = {}\n",
        "\n",
        "\n",
        "    def read_dataset(self):\n",
        "        \"\"\"\n",
        "        Reads the dataset from the given path.\n",
        "        \"\"\"\n",
        "        ret = pd.read_csv(self.path)\n",
        "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "        # rearange the columns as funny, joke\n",
        "        ret = ret[['funny', 'joke']]\n",
        "        ret = ret.sample(frac=1).reset_index(drop=True)\n",
        "        # return ret\n",
        "        self.data = ret\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the text data.\n",
        "        \"\"\"\n",
        "        # gensim.utils.simple_preprocess. \n",
        "        # This will remove all punctuation, remove stop words and tokenize the given sentence.\n",
        "        self.data['tokens'] = self.data['joke'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
        "\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Gets the vocabulary.\n",
        "        \"\"\"\n",
        "        self.vocabulary_size = len(self.data['tokens'].apply(set).apply(len))\n",
        "        self.vocabulary = self.data['tokens'].apply(set).apply(list)\n",
        "\n",
        "\n",
        "    def get_max_tokens(self):\n",
        "        \"\"\"\n",
        "        Gets the maximum number of tokens in a joke.\n",
        "        \"\"\"\n",
        "        self.data['max_tokens'] = 0\n",
        "        # create a new column in the dataframe with max tokens per row. and count only if the token isalpha()\n",
        "        for index, row in self.data.iterrows():\n",
        "            count = 0\n",
        "            for token in row['tokens']:\n",
        "                if token.isalpha():\n",
        "                    count += 1\n",
        "            self.data['max_tokens'][index] = count\n",
        "        \n",
        "        # Get the Vocabulary\n",
        "        self.get_vocabulary()\n",
        "\n",
        "\n",
        "    def w2v_model(self, max_length):\n",
        "        \"\"\"\n",
        "        Splits the data into train and validation sets.\n",
        "        Constructs the word2vec model. (Feature vector)\n",
        "        \"\"\"\n",
        "        X = self.data['tokens']\n",
        "        y = self.data['funny']\n",
        "\n",
        "        print(\"X shape: \", X.shape)\n",
        "        print(\"y shape: \", y.shape)\n",
        "        # split the data into train and validation sets and make them random\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
        "\n",
        "        w2v_model = Word2Vec(X_train, vector_size=max_length, min_count=1, window=self.context_window)\n",
        "\n",
        "        w2v_model.train(X_train, total_examples=len(X_train), epochs=10)\n",
        "\n",
        "        vocab = w2v_model.wv.index_to_key\n",
        "        print(\"Vocabulary size: \", len(vocab))\n",
        "\n",
        "        # self.word_vec = {}\n",
        "        for word in vocab:\n",
        "            self.word_vec[word] = w2v_model.wv.get_vector(word)\n",
        "        \n",
        "        # print(word_vec['like'])\n",
        "        print(\"The no of key-value pairs : \",len(self.word_vec))\n",
        "\n",
        "\n",
        "    def LSTM_model(self, max_length):\n",
        "        # Create a padded sequence of the joke\n",
        "        # tokeniser = Tokenizer()\n",
        "        self.tokeniser.fit_on_texts(self.data['tokens'])\n",
        "        vocab_size = len(self.tokeniser.word_index) + 1\n",
        "        print(\"Vocabulary size: \", vocab_size)\n",
        "\n",
        "        # pad the sequences to the same length\n",
        "        ret = self.tokeniser.texts_to_sequences(self.data['tokens'])\n",
        "\n",
        "        data_padded = pad_sequences(ret, maxlen=max_length, padding='post')\n",
        "        print(\"Data padded shape: \", data_padded.shape)\n",
        "\n",
        "        # Create the embeddings matrix\n",
        "        embedding_matrix = np.zeros((vocab_size, max_length))\n",
        "        for word, i in self.tokeniser.word_index.items():\n",
        "            embedding_vector = self.word_vec.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        # print(embedding_matrix[15])\n",
        "\n",
        "        Y = self.data['funny']\n",
        "        x_train, x_test, y_train, y_test = train_test_split(data_padded, \n",
        "                                                            Y, test_size=config['test_p'], \n",
        "                                                            random_state=42)\n",
        "\n",
        "        # Building the model\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Embedding(input_dim=vocab_size, output_dim=max_length, input_length=max_length, \n",
        "                            weights=[embedding_matrix], trainable=False))\n",
        "        self.model.add(Dropout(config['Dropout_layer'][0]))\n",
        "        self.model.add(LSTM(config['LSTM_layer'][1], activation=config['activation'])) #dropout=config['Dropout_layer'][1], recurrent_dropout=0.2))\n",
        "        self.model.add(Dropout(config['Dropout_layer'][1]))\n",
        "        self.model.add(Dense(units=1, activation=config['activation']))\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(x_train, y_train,epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto', validation_split=config['val_p'])\n",
        "        self.model.save(config['model_path']) # save the model\n",
        "        self.evaluate_model(x_test, y_test, max_length) # evaluate the model\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test, max_length):\n",
        "        \"\"\"\n",
        "        Evaluates the model.\n",
        "        \"\"\"\n",
        "        scores = self.model.evaluate(X_test, y_test)\n",
        "        print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "        # Print Precision and Recall\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        y_pred = np.round(y_pred)\n",
        "\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        print(\"Precision: %.2f%%\" % (precision * 100))\n",
        "        print(\"Recall: %.2f%%\" % (recall * 100))\n",
        "        print(\"F1-Score: %.2f%%\" % (f1 * 100))\n",
        "\n",
        "        # Save data\n",
        "        self.save_data(max_length, scores, precision, recall, f1)\n",
        "\n",
        "    def save_data(self, max_length, accuracy, precision, recall, f1):\n",
        "        \"\"\"\n",
        "        Saves the data.\n",
        "        \"\"\"\n",
        "        # Add in dataframe master_df max_len, accuracy, precision, recall, f1-score\n",
        "        ret = pd.DataFrame(columns=['max_len', 'accuracy', 'precision', 'recall', 'f1-score'])\n",
        "        ret.loc[0] = [max_length, accuracy, precision, recall, f1]\n",
        "        ret.to_csv(config['data_path'])\n",
        "        \n",
        "        # save the tokenizer\n",
        "        with open(config['tokenizer_path'], 'wb') as handle:\n",
        "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCHGiS2LbQbZ"
      },
      "source": [
        "### Read Data and pre-process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPmtwU98bPvi",
        "outputId": "bdd0f0bd-abfc-4fc2-b6fc-fce883eca7b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\paulc\\AppData\\Local\\Temp/ipykernel_21060/246516969.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.data['max_tokens'][index] = count\n"
          ]
        }
      ],
      "source": [
        "# SETTINGS for local machine - change this for Goolg Colab\n",
        "path = \"dataset/final_jokes(1283).csv\"\n",
        "# path = \"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "# joke_model.data = \n",
        "joke_model.read_dataset()\n",
        "joke_model.preprocess_text()\n",
        "\n",
        "joke_model.get_max_tokens() # get the maximum number of tokens. Since we need the word2vec feature vector to be of the same size for all jokes. \n",
        "max_length_joke = joke_model.data['max_tokens'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlvuGfA1bTjV"
      },
      "source": [
        "### Construct the word2vec embedding vector and train the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX9FyZMl1K3F",
        "outputId": "17d042d3-8d2b-4998-8304-610226b798d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape:  (1283,)\n",
            "y shape:  (1283,)\n",
            "Vocabulary size:  3547\n",
            "The no of key-value pairs :  3547\n",
            "Vocabulary size:  4647\n",
            "Data padded shape:  (1283, 332)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 332, 332)          1542804   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 332, 332)          0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 100)               173200    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,716,105\n",
            "Trainable params: 173,301\n",
            "Non-trainable params: 1,542,804\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "14/14 [==============================] - 24s 1s/step - loss: 0.6931 - accuracy: 0.5406 - val_loss: 0.6932 - val_accuracy: 0.4854\n",
            "Epoch 2/3\n",
            "14/14 [==============================] - 13s 896ms/step - loss: 0.6927 - accuracy: 0.5406 - val_loss: 0.6934 - val_accuracy: 0.4854\n",
            "Epoch 3/3\n",
            "14/14 [==============================] - 10s 743ms/step - loss: 0.6923 - accuracy: 0.5406 - val_loss: 0.6937 - val_accuracy: 0.4854\n",
            "9/9 [==============================] - 2s 180ms/step - loss: 0.6926 - accuracy: 0.5214\n",
            "Accuracy: 52.14%\n",
            "Precision: 52.14%\n",
            "Recall: 100.00%\n",
            "F1-Score: 68.54%\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'LSTM_model' object has no attribute 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21060/1554976200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mjoke_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length_joke\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjoke_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length_joke\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21060/246516969.py\u001b[0m in \u001b[0;36mLSTM_model\u001b[1;34m(self, max_length)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21060/246516969.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(self, X_test, y_test, max_length)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;31m# Save data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21060/246516969.py\u001b[0m in \u001b[0;36msave_data\u001b[1;34m(self, max_length, accuracy, precision, recall, f1)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# save the tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenizer_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: 'LSTM_model' object has no attribute 'tokenizer'"
          ]
        }
      ],
      "source": [
        "joke_model.w2v_model(max_length_joke)\n",
        "joke_model.LSTM_model(max_length_joke)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2c_word2vec.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a3a52a35f691380e40dee258b5f482ebea5e8d938b654dd4fe14bb13e1a5bfa7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
