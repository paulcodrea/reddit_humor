{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from time import time\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout \n",
    "from keras.layers import LSTM, Embedding\n",
    "# from time import time\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from pathlib import Path\n",
    "\n",
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import nltk\n",
    "# import regex as re\n",
    "# from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might have to change the following\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 5, \n",
    "    \"batch_size\": 32,\n",
    "    \"train_p\": 0.55,\n",
    "    \"val_p\": 0.05,\n",
    "    \"LSTM_layer\": [50, 100],\n",
    "    \"Dropout_layer\": [0.15, 0.2],\n",
    "    \"activation\": 'tanh',\n",
    "    \"timesteps\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model:\n",
    "    def __init__(self, path):\n",
    "        self.path = path # Path to the dataset\n",
    "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
    "\n",
    "        self.context_window = 3 # Context window size\n",
    "        self.w2v_feature_vector = []\n",
    "        self.vocabulary_size = 0\n",
    "        self.vocabulary = []\n",
    "\n",
    "        self.jokes_to_numerical = []\n",
    "        self.model = None\n",
    "\n",
    "    def read_dataset(self):\n",
    "        \"\"\"\n",
    "        Reads the dataset from the given path.\n",
    "        \"\"\"\n",
    "        ret = pd.read_csv(self.path)\n",
    "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def preprocess_text(self):\n",
    "        \"\"\"\n",
    "        Preprocesses the text data.\n",
    "        \"\"\"\n",
    "        self.data['tokens'] = self.data['joke'].apply(word_tokenize) # tokenize the text but keep the punctuation\n",
    "\n",
    "    # get the maximum size of tokens in the dataset and add to column\n",
    "    def get_max_tokens(self):\n",
    "        self.data['max_tokens'] = self.data['tokens'].apply(lambda x: len(x))\n",
    "\n",
    "        # run get_vocalulary() again to get the vocabulary\n",
    "        self.get_vocabulary()\n",
    "\n",
    "\n",
    "    def construct_word2vec(self, max_length):\n",
    "        \"\"\"\n",
    "        Constructs the word2vec model. (Feature vector)\n",
    "        \"\"\"\n",
    "        self.w2v_feature_vector = []\n",
    "        context_words = [] # Construct window list for word2vec\n",
    "        \n",
    "        for line in self.data['tokens']:\n",
    "            for index, word in enumerate(line):\n",
    "                if self.context_window > 0:\n",
    "                    left = index - self.context_window//2\n",
    "                    right = index + self.context_window//2 + 1\n",
    "                else:\n",
    "                    left = index - self.context_window//2\n",
    "                    right = index + self.context_window//2\n",
    "                context_words.append([line[i] for i in range(left, right) if i >= 0 and i < len(line)])\n",
    " \n",
    "\n",
    "        # Create a word2vec model\n",
    "        # context_words = [['a', 'b'], ['a', 'b', 'c'], ['b', 'c', 'd'], ['c', 'd', 'e'], ['d', 'e']] -> list of lists of words and window size is 5\n",
    "        # vector_size = 50 -> dimension of the feature vector (pairs)\n",
    "        # min_count = 4 -> minimum number of occurrences of a word in the corpus\n",
    "        # workers = 4 -> number of threads to use\n",
    "        # window = 5 -> window size\n",
    "        model = Word2Vec(context_words, vector_size=max_length, window=self.context_window, workers=4)\n",
    "\n",
    "        for line in self.data['tokens']:\n",
    "            for index, word in enumerate(line):\n",
    "                if word in model.wv.key_to_index:\n",
    "                    self.w2v_feature_vector.append(model.wv.get_vector(word))\n",
    "                else:\n",
    "                    # if the word is not in the model, then add zero. \n",
    "                    self.w2v_feature_vector.append(np.zeros(max_length))\n",
    "\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Gets the vocabulary.\n",
    "        \"\"\"\n",
    "        self.vocabulary_size = len(self.data['tokens'].apply(set).apply(len))\n",
    "        self.vocabulary = self.data['tokens'].apply(set).apply(list)\n",
    "\n",
    "\n",
    "    # def data_split(self):\n",
    "    #     \"\"\"\n",
    "    #     Splits the data into train and validation sets.\n",
    "\n",
    "    #     Constructs the LSTM model.\n",
    "    #     \"\"\"\n",
    "    #     X = self.jokes_to_numerical\n",
    "    #     y = self.data['funny']\n",
    "\n",
    "    #     print(\"X shape: \", X.shape)\n",
    "    #     print(\"y shape: \", y.shape)\n",
    "    #     # split the data into train and validation sets and make them random\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "    #     # print size of the train and test sets\n",
    "    #     print(\"X_train:\", len(X_train))\n",
    "    #     print(\"X_test:\", len(X_test))\n",
    "    #     print(\"y_train:\", len(y_train))\n",
    "    #     print(\"y_test:\", len(y_test))\n",
    "\n",
    "    #     self.model = Sequential()\n",
    "    #     self.model.add(Embedding(input_dim=self.vocabulary_size, output_dim=32, input_length=405))#X.shape[2]))\n",
    "    #     # self.model.add(LSTM(config['LSTM_layer'][0], activation=config['activation'], input_shape=(X.shape[1])))\n",
    "    #     self.model.add(Dropout(config['Dropout_layer'][0]))\n",
    "    #     self.model.add(LSTM(config['LSTM_layer'][1], activation=config['activation']))\n",
    "    #     self.model.add(Dropout(config['Dropout_layer'][1]))\n",
    "    #     self.model.add(Dense(units=1, activation='softmax'))\n",
    "    #     self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #     self.model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto')#, validation_split=config['val_p'])\n",
    "\n",
    "    #     # Evaluate the model\n",
    "    #     scores = self.model.evaluate(X_test, y_test)\n",
    "    #     print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of joke:  405\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS for local machine - change this for Goolg Colab\n",
    "path = \"dataset/final_jokes(1283).csv\" #\"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
    "\n",
    "joke_model = LSTM_model(path)\n",
    "joke_model.data = joke_model.read_dataset()\n",
    "\n",
    "joke_model.preprocess_text()\n",
    "joke_model.get_max_tokens() # get the maximum number of tokens. Since we need the word2vec feature vector to be of the same size for all jokes. \n",
    "max_length_joke = joke_model.data['max_tokens'].max()\n",
    "print(\"Max length of joke: \", max_length_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "joke_model.construct_word2vec(max_length_joke)\n",
    "\n",
    "print(len(joke_model.data['tokens'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93263193325cd6f76a9e21d176003adfa7a63d8f308e38218ffc45e6019c4146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
