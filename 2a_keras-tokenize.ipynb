{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kTI_jxV_7R56"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from time import time\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding\n",
        "# from time import time\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from pathlib import Path\n",
        "\n",
        "# train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import nltk\n",
        "# import regex as re\n",
        "# from collections import defaultdict\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# import Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# from nltk.stem.snowball import EnglishStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZzV9t8t87ZOw"
      },
      "outputs": [],
      "source": [
        "# We might have to change the following\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 5, \n",
        "    \"batch_size\": 32,\n",
        "    \"train_p\": 0.55,\n",
        "    \"val_p\": 0.05,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'tanh',\n",
        "    \"timesteps\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "    def __init__(self, path):\n",
        "        self.path = path # Path to the dataset\n",
        "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
        "\n",
        "        self.context_window = 3 # Context window size\n",
        "        self.w2v_feature_vector = []\n",
        "        self.vocabulary_size = 0\n",
        "\n",
        "        self.jokes_to_numerical = []\n",
        "        self.model = None\n",
        "\n",
        "    def read_dataset(self):\n",
        "        \"\"\"\n",
        "        Reads the dataset from the given path.\n",
        "        \"\"\"\n",
        "        ret = pd.read_csv(self.path)\n",
        "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "        # randomize the data set and take the first 1000 rows\n",
        "        ret = ret.sample(frac=1)#.head(1000)\n",
        "\n",
        "        self.data = ret\n",
        "\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the text data.\n",
        "        \"\"\"\n",
        "        self.data['tokens'] = self.data['joke'].apply(word_tokenize) # tokenize the text but keep the punctuation\n",
        "\n",
        "    # get the maximum size of tokens in the dataset and add to column\n",
        "    def get_max_tokens(self):\n",
        "        self.data['max_tokens'] = self.data['tokens'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "    def convert_jokes_to_numerical(self):\n",
        "        \"\"\"\n",
        "        Converts the jokes to numerical values.\n",
        "        \"\"\"\n",
        "        tokenizer = Tokenizer(num_words=None, split=' ')\n",
        "        tokenizer.fit_on_texts(self.data['joke'].values)\n",
        "        self.jokes_to_numerical = tokenizer.texts_to_sequences(self.data['joke'].values)\n",
        "\n",
        "        # get vocabulary size\n",
        "        self.vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    def pad_sequences(self, max_length):\n",
        "        \"\"\"\n",
        "        Pads the sequences.\n",
        "        \"\"\"\n",
        "        self.jokes_to_numerical = pad_sequences(self.jokes_to_numerical, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "    def data_split(self):\n",
        "        \"\"\"\n",
        "        Splits the data into train and validation sets.\n",
        "\n",
        "        Constructs the LSTM model.\n",
        "        \"\"\"\n",
        "        X = self.jokes_to_numerical\n",
        "        y = self.data['funny']\n",
        "\n",
        "        print(\"X shape: \", X.shape)\n",
        "        print(\"y shape: \", y.shape)\n",
        "        # split the data into train and validation sets and make them random\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # print(X_train[0])\n",
        "\n",
        "        print(\"X_train:\", len(X_train))\n",
        "        print(\"X_test:\", len(X_test))\n",
        "        print(\"y_train:\", len(y_train))\n",
        "        print(\"y_test:\", len(y_test))\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Embedding(input_dim=self.vocabulary_size, output_dim=120, input_length=int(X.shape[1])))\n",
        "        # self.model.add(LSTM(config['LSTM_layer'][0], activation=config['activation'], input_shape=(X.shape[1])))\n",
        "        self.model.add(Dropout(config['Dropout_layer'][0]))\n",
        "        self.model.add(LSTM(config['LSTM_layer'][1], activation=config['activation']))\n",
        "        self.model.add(Dropout(config['Dropout_layer'][1]))\n",
        "        self.model.add(Dense(units=1, activation='softmax'))\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        self.model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto')#, validation_split=config['val_p'])\n",
        "\n",
        "        # Evaluate the model\n",
        "        scores = self.model.evaluate(X_test, y_test)\n",
        "        print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJL35QEBy-Jz",
        "outputId": "2151be98-c647-49af-d7d4-76e7a6ea5fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of joke:  405\n"
          ]
        }
      ],
      "source": [
        "# SETTINGS for local machine - change this for Goolg Colab\n",
        "path = \"dataset/final_jokes(1283).csv\" #\"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.read_dataset()\n",
        "\n",
        "joke_model.preprocess_text()\n",
        "joke_model.get_max_tokens() # get the maximum number of tokens. Since we need the word2vec feature vector to be of the same size for all jokes. \n",
        "max_length_joke = joke_model.data['max_tokens'].max()\n",
        "print(\"Max length of joke: \", max_length_joke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer from keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of first line:  186\n",
            "Length of feature vector after normalisation:  405\n"
          ]
        }
      ],
      "source": [
        "joke_model.convert_jokes_to_numerical()\n",
        "print(\"Length of first line: \", len(joke_model.data['joke'][0]))\n",
        "\n",
        "joke_model.pad_sequences(max_length_joke)\n",
        "print(\"Length of feature vector after normalisation: \", len(joke_model.jokes_to_numerical[0]))\n",
        "\n",
        "# print(joke_model.jokes_to_numerical[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape:  (1283, 405)\n",
            "y shape:  (1283,)\n",
            "X_train: 898\n",
            "X_test: 385\n",
            "y_train: 898\n",
            "y_test: 385\n",
            "Epoch 1/5\n",
            "29/29 [==============================] - 16s 382ms/step - loss: 0.6936 - accuracy: 0.5356\n",
            "Epoch 2/5\n",
            "29/29 [==============================] - 11s 367ms/step - loss: 0.6924 - accuracy: 0.5356\n",
            "Epoch 3/5\n",
            "29/29 [==============================] - 11s 376ms/step - loss: 0.6934 - accuracy: 0.5356\n",
            "Epoch 4/5\n",
            "29/29 [==============================] - 13s 442ms/step - loss: 0.6900 - accuracy: 0.5356\n",
            "Epoch 5/5\n",
            "29/29 [==============================] - 12s 425ms/step - loss: 0.6915 - accuracy: 0.5356\n",
            "13/13 [==============================] - 3s 162ms/step - loss: 0.6924 - accuracy: 0.5247\n",
            "Accuracy: 52.47%\n"
          ]
        }
      ],
      "source": [
        "joke_model.data_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJ+LSjkMXZfSgYMmoTLpK7",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1j9kXqN1mbP4p8UZmRZX4T4ZLOsXsnFnk",
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
