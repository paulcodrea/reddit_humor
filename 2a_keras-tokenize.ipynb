{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcodrea/reddit_humor/blob/main/2a_keras-tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTI_jxV_7R56",
        "outputId": "e7b42c12-5733-43f5-d117-e0fc34437707"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\paulc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Dropout \n",
        "from keras.layers import LSTM, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvpRiHqs53Vu",
        "outputId": "ccc504f4-a943-47e8-dcf2-cdbfdbcec83e"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZzV9t8t87ZOw"
      },
      "outputs": [],
      "source": [
        "# We might have to change the following\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epochs\": 3, \n",
        "    \"batch_size\": 70,\n",
        "    \"test_p\": 0.2,\n",
        "    \"val_p\": 0.1,\n",
        "    \"LSTM_layer\": [50, 100],\n",
        "    \"Dropout_layer\": [0.15, 0.2],\n",
        "    \"activation\": 'softmax',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_McP__5n5US1"
      },
      "outputs": [],
      "source": [
        "class LSTM_model:\n",
        "    def __init__(self, path):\n",
        "        \"\"\"\n",
        "        Initializes the class.\n",
        "        \"\"\"\n",
        "        self.path = path # Path to the dataset\n",
        "        self.data = pd.DataFrame() # Dataframe to store the dataset\n",
        "\n",
        "        # self.context_window = 3 # Context window size\n",
        "        # self.w2v_feature_vector = []\n",
        "        self.vocabulary_size = 0\n",
        "\n",
        "        self.jokes_to_numerical = []\n",
        "        self.model = None\n",
        "\n",
        "\n",
        "    def read_dataset(self):\n",
        "        \"\"\"\n",
        "        Reads the dataset from the given path.\n",
        "        \"\"\"\n",
        "        ret = pd.read_csv(self.path)\n",
        "        ret.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "        # randomize the data set and take the first 10 rows\n",
        "        ret = ret.sample(frac=1, ignore_index=True)#.head(10)\n",
        "        self.data = ret\n",
        "\n",
        "\n",
        "    def emoji_tokenizer(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes the text. Removes the emojis.\n",
        "        \"\"\"\n",
        "        return [c for c in text if c not in emoji.UNICODE_EMOJI]\n",
        "\n",
        "        \n",
        "    def preprocess_text(self):\n",
        "        \"\"\"\n",
        "        Preprocesses the text data.\n",
        "        \"\"\"\n",
        "        # Remove URLs.\n",
        "        self.data['text'] = self.data['joke'].apply(lambda x: ' '.join(word for word in x.split() if not word.startswith('http')))\n",
        "\n",
        "        # # Remove rows if text contains \"[ Removed\" or \"[Removed\"\n",
        "        # self.data = self.data[self.data['text'].str.contains(\"[ Removed\") == False]\n",
        "        # self.data = self.data[self.data['text'].str.contains(\"[Removed\") == False]\n",
        "\n",
        "        # Remove emojis.\n",
        "        self.data['text'] = self.data['text'].apply(self.emoji_tokenizer)\n",
        "\n",
        "        self.data['tokens'] = self.data['text'].apply(word_tokenize)\n",
        "        # self.data['tokens'] = self.data['joke'].apply(word_tokenize) # tokenize the text but keep the punctuation\n",
        "\n",
        "\n",
        "    def get_max_tokens(self):\n",
        "        \"\"\"\n",
        "        Returns the maximum number of tokens in the dataset.\n",
        "        \"\"\"\n",
        "        self.data['max_tokens'] = 0\n",
        "        # create a new column in the dataframe with max tokens per row. and count only if the token isalpha()\n",
        "        for index, row in self.data.iterrows():\n",
        "            count = 0\n",
        "            for token in row['tokens']:\n",
        "                if token.isalpha():\n",
        "                    count += 1\n",
        "            self.data['max_tokens'][index] = count\n",
        "\n",
        "\n",
        "    def convert_jokes_to_numerical(self):\n",
        "        \"\"\"\n",
        "        Converts the jokes to numerical values.\n",
        "        \"\"\"\n",
        "        tokenizer = Tokenizer(num_words=None, split=' ')\n",
        "        tokenizer.fit_on_texts(self.data['joke'].values)\n",
        "        self.jokes_to_numerical = tokenizer.texts_to_sequences(self.data['joke'].values)\n",
        "\n",
        "        # get vocabulary size\n",
        "        self.vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "    def pad_sequences(self, max_length):\n",
        "        \"\"\"\n",
        "        Pads the sequences.\n",
        "        \"\"\"\n",
        "        self.jokes_to_numerical = pad_sequences(self.jokes_to_numerical, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "    def LSTM_model(self, max_length):\n",
        "        \"\"\"\n",
        "        Splits the data into train and validation sets.\n",
        "        Constructs the LSTM model.\n",
        "        \"\"\"\n",
        "        X = self.jokes_to_numerical\n",
        "        y = self.data['funny']\n",
        "\n",
        "        print(\"X shape: \", X.shape)\n",
        "        print(\"y shape: \", y.shape)\n",
        "\n",
        "        # split the data into train and validation sets and make them random\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config['test_p'], random_state=42)\n",
        "\n",
        "        print(\"X_train:\", len(X_train))\n",
        "        print(\"X_test:\", len(X_test))\n",
        "        print(\"y_train:\", len(y_train))\n",
        "        print(\"y_test:\", len(y_test))\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Embedding(input_dim=self.vocabulary_size, output_dim=max_length, input_length=int(X.shape[1])))\n",
        "        self.model.add(Dropout(config['Dropout_layer'][0]))\n",
        "        self.model.add(LSTM(config['LSTM_layer'][1], activation=config['activation']))\n",
        "        self.model.add(Dropout(config['Dropout_layer'][1]))\n",
        "        self.model.add(Dense(units=1, activation=config['activation']))\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        self.model.fit(X_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'], verbose='auto', validation_split=config['val_p'])\n",
        "\n",
        "        # evaluate the model\n",
        "        self.evaluate_model(X_test, y_test)\n",
        "\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model.\n",
        "        \"\"\"\n",
        "        scores = self.model.evaluate(X_test, y_test)\n",
        "        print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "        # Print Precision and Recall\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        y_pred = np.round(y_pred)\n",
        "        print(\"Precision: %.2f%%\" % (precision_score(y_test, y_pred) * 100))\n",
        "        print(\"Recall: %.2f%%\" % (recall_score(y_test, y_pred) * 100))\n",
        "        print(\"F1-Score: %.2f%%\" % (f1_score(y_test, y_pred) * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read data and pre-process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NRyY8hOIG6hX"
      },
      "outputs": [],
      "source": [
        "# SETTINGS for local machine - change this for Goolg Colab\n",
        "path = \"dataset/final_jokes(1283).csv\" \n",
        "# path = \"/content/drive/MyDrive/NLU_Humor-detection/final_jokes(1283).csv\"\n",
        "\n",
        "joke_model = LSTM_model(path)\n",
        "joke_model.read_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJL35QEBy-Jz",
        "outputId": "6358218b-1862-4189-da7e-11556634d9e9"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "unterminated character set at position 0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12988/3264640184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjoke_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mjoke_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_max_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmax_length_joke\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoke_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_______________________________________________________________________\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nMax length of joke: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_joke\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12988/2226358948.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Remove rows if text contains \"[ Removed\" or \"[Removed\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[ Removed\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Removed\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 )\n\u001b[0;32m    115\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36mcontains\u001b[1;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \"\"\"\n\u001b[1;32m-> 1153\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m             warnings.warn(\n\u001b[0;32m   1155\u001b[0m                 \u001b[1;34m\"This pattern has match groups. To actually get the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\re.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[0;32m    444\u001b[0m                            not nested and not items))\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    547\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msourceget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                     raise source.error(\"unterminated character set\",\n\u001b[0m\u001b[0;32m    550\u001b[0m                                        source.tell() - here)\n\u001b[0;32m    551\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"]\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31merror\u001b[0m: unterminated character set at position 0"
          ]
        }
      ],
      "source": [
        "joke_model.preprocess_text()\n",
        "joke_model.get_max_tokens() \n",
        "max_length_joke = joke_model.data['max_tokens'].max()\n",
        "print(\"_______________________________________________________________________\")\n",
        "print(\"\\nMax length of joke: \", max_length_joke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Construct the embeddings vector for the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of feature vector after normalisation:  329\n"
          ]
        }
      ],
      "source": [
        "joke_model.convert_jokes_to_numerical()\n",
        "\n",
        "joke_model.pad_sequences(max_length_joke)\n",
        "print(\"Length of feature vector after normalisation: \", \n",
        "      len(joke_model.jokes_to_numerical[0]))\n",
        "\n",
        "\n",
        "# joke_model.jokes_to_numerical[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4951\n"
          ]
        }
      ],
      "source": [
        "print(joke_model.vocabulary_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhZV9sGA5UTH",
        "outputId": "3e0c3bb2-516c-455d-dace-c81aa6826b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape:  (1283, 329)\n",
            "y shape:  (1283,)\n",
            "X Train shape (1026, 329)\n",
            "y train shape (1026,)\n",
            "X_train: 1026\n",
            "X_test: 257\n",
            "y_train: 1026\n",
            "y_test: 257\n",
            "Epoch 1/3\n",
            "14/14 [==============================] - 11s 714ms/step - loss: 0.6935 - accuracy: 0.5157 - val_loss: 0.6942 - val_accuracy: 0.6019\n",
            "Epoch 2/3\n",
            "14/14 [==============================] - 12s 869ms/step - loss: 0.6934 - accuracy: 0.5157 - val_loss: 0.6936 - val_accuracy: 0.6019\n",
            "Epoch 3/3\n",
            "14/14 [==============================] - 13s 898ms/step - loss: 0.6932 - accuracy: 0.5157 - val_loss: 0.6930 - val_accuracy: 0.6019\n",
            "9/9 [==============================] - 1s 129ms/step - loss: 0.6931 - accuracy: 0.5642\n",
            "Accuracy: 56.42%\n",
            "Precision: 56.42%\n",
            "Recall: 100.00%\n",
            "F1-Score: 72.14%\n"
          ]
        }
      ],
      "source": [
        "joke_model.LSTM_model(max_length_joke)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a3a52a35f691380e40dee258b5f482ebea5e8d938b654dd4fe14bb13e1a5bfa7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
